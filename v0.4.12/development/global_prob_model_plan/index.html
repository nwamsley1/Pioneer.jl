<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Global Precursor Probability Model — Design and Implementation Plan · Pioneer.jl</title><meta name="title" content="Global Precursor Probability Model — Design and Implementation Plan · Pioneer.jl"/><meta property="og:title" content="Global Precursor Probability Model — Design and Implementation Plan · Pioneer.jl"/><meta property="twitter:title" content="Global Precursor Probability Model — Design and Implementation Plan · Pioneer.jl"/><meta name="description" content="Documentation for Pioneer.jl."/><meta property="og:description" content="Documentation for Pioneer.jl."/><meta property="twitter:description" content="Documentation for Pioneer.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Pioneer.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">User Guide</span><ul><li><a class="tocitem" href="../../user_guide/installation/">Installation Guide</a></li><li><a class="tocitem" href="../../user_guide/quickstart/">Quick Start Tutorial</a></li><li><a class="tocitem" href="../../user_guide/parameters/">Parameter Configuration</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Global Precursor Probability Model — Design and Implementation Plan</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Global Precursor Probability Model — Design and Implementation Plan</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/nwamsley1/Pioneer.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/nwamsley1/Pioneer.jl/blob/develop/docs/src/development/global_prob_model_plan.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Global-Precursor-Probability-Model-—-Design-and-Implementation-Plan"><a class="docs-heading-anchor" href="#Global-Precursor-Probability-Model-—-Design-and-Implementation-Plan">Global Precursor Probability Model — Design and Implementation Plan</a><a id="Global-Precursor-Probability-Model-—-Design-and-Implementation-Plan-1"></a><a class="docs-heading-anchor-permalink" href="#Global-Precursor-Probability-Model-—-Design-and-Implementation-Plan" title="Permalink"></a></h1><h2 id="Goal"><a class="docs-heading-anchor" href="#Goal">Goal</a><a id="Goal-1"></a><a class="docs-heading-anchor-permalink" href="#Goal" title="Permalink"></a></h2><p>Replace the current heuristic <code>:global_prob</code> aggregation (log-odds of per-run <code>:prec_prob</code>) with a dedicated ML model trained at the precursor level. The model predicts a single experiment‑wide probability per <code>precursor_idx</code> using summary features aggregated across runs. The predicted probability becomes the new <code>:global_prob</code> and is assigned back to all PSM rows for that <code>precursor_idx</code> in <code>merged_df</code> and downstream artifacts.</p><h2 id="Rationale"><a class="docs-heading-anchor" href="#Rationale">Rationale</a><a id="Rationale-1"></a><a class="docs-heading-anchor-permalink" href="#Rationale" title="Permalink"></a></h2><ul><li>The present <code>logodds(p, sqrt_n_runs)</code> is simple and robust but cannot exploit richer cross‑run signals (e.g., score dispersion, coverage, consistency).</li><li>A small tree‑based model (LightGBM, same backend used elsewhere) can learn non‑linearities and interactions from a compact, per‑precursor feature vector, improving separation of targets and decoys at the precursor level.</li></ul><h2 id="Scope"><a class="docs-heading-anchor" href="#Scope">Scope</a><a id="Scope-1"></a><a class="docs-heading-anchor-permalink" href="#Scope" title="Permalink"></a></h2><ul><li>In scope: New aggregation machinery, training pipeline, cross‑validated fit, prediction to all <code>precursor_idx</code>, and integration into ScoringSearch’s pipeline to populate <code>:global_prob</code>.</li><li>Out of scope: Changes to per‑run <code>:prec_prob</code> estimation, downstream q‑value/PEP logic beyond switching <code>:global_prob</code> source, model persistence across projects.</li></ul><h2 id="Data-and-Features"><a class="docs-heading-anchor" href="#Data-and-Features">Data and Features</a><a id="Data-and-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Data-and-Features" title="Permalink"></a></h2><h3 id="Source-tables"><a class="docs-heading-anchor" href="#Source-tables">Source tables</a><a id="Source-tables-1"></a><a class="docs-heading-anchor-permalink" href="#Source-tables" title="Permalink"></a></h3><ul><li><code>merged_df</code> after Step 2/3 of ScoringSearch: has one row per <code>(precursor_idx, ms_file_idx)</code> with columns at minimum <code>:precursor_idx</code>, <code>:prec_prob</code>, <code>:target</code>, possibly <code>:trace_prob</code>, etc.</li></ul><h3 id="Aggregation-object-(custom-type)"><a class="docs-heading-anchor" href="#Aggregation-object-(custom-type)">Aggregation object (custom type)</a><a id="Aggregation-object-(custom-type)-1"></a><a class="docs-heading-anchor-permalink" href="#Aggregation-object-(custom-type)" title="Permalink"></a></h3><p>Define a compact summary per <code>precursor_idx</code>. We will construct a Dictionary keyed by <code>precursor_idx</code> whose values are instances of a small immutable struct:</p><pre><code class="language-julia hljs">struct GlobalPrecFeatures{N}
    top_scores::NTuple{N, Float32}  # descending top-N :prec_prob across runs (pad with 0.0f0)
    stats::NTuple{5, Float32}       # (mean, max, min, std, iqr)
    counts::NTuple{3, Int32}        # (n_runs_with_score, n_runs_total, n_above_thresh)
    deltas::NTuple{2, Float32}      # (top1 - top2, top2 - top3)
    logodds_baseline::Float32       # current baseline (for ensembling/ablation)
end</code></pre><p>Notes:</p><ul><li><code>N</code> (default 5) controls the number of highest scores retained; pad missing with zeros.</li><li><code>n_runs_total</code> equals the number of MS files considered for the experiment.</li><li><code>n_above_thresh</code> uses a configurable <code>prec_prob</code> threshold (e.g., 0.95) to capture run coverage quality.</li><li><code>iqr</code> computed from observed per‑run scores for robustness; if &lt;4 observations, fallback to 0.</li><li><code>logodds_baseline</code> equals the existing <code>logodds(p, sqrt_n_runs)</code> result for a comparable reference feature.</li></ul><h3 id="Label-assignment"><a class="docs-heading-anchor" href="#Label-assignment">Label assignment</a><a id="Label-assignment-1"></a><a class="docs-heading-anchor-permalink" href="#Label-assignment" title="Permalink"></a></h3><ul><li>Precursor label <code>target::Bool</code> taken from library metadata (same across runs) via existing lookup on <code>precursor_idx</code>.</li></ul><h3 id="Feature-engineering-details"><a class="docs-heading-anchor" href="#Feature-engineering-details">Feature engineering details</a><a id="Feature-engineering-details-1"></a><a class="docs-heading-anchor-permalink" href="#Feature-engineering-details" title="Permalink"></a></h3><ul><li>Sorting and padding for <code>top_scores</code> guarantee fixed‑length inputs for the model.</li><li>Standardization is not required for tree‑based models, but we will clamp/clip probabilities into <code>[eps, 1-eps]</code> when derived metrics use logs (e.g., baseline log‑odds).</li></ul><h2 id="Training-Pipeline"><a class="docs-heading-anchor" href="#Training-Pipeline">Training Pipeline</a><a id="Training-Pipeline-1"></a><a class="docs-heading-anchor-permalink" href="#Training-Pipeline" title="Permalink"></a></h2><h3 id="Dataset-construction"><a class="docs-heading-anchor" href="#Dataset-construction">Dataset construction</a><a id="Dataset-construction-1"></a><a class="docs-heading-anchor-permalink" href="#Dataset-construction" title="Permalink"></a></h3><ol><li>Group <code>merged_df</code> by <code>:precursor_idx</code> and collect <code>:prec_prob</code> values (one per run where observed).</li><li>Build <code>GlobalPrecFeatures{N}</code> per group and record <code>target</code>.</li><li>Convert the dictionary to a unique DataFrame with one row per <code>precursor_idx</code> and columns:<ul><li><code>precursor_idx::UInt32</code>, feature columns expanded from the struct, and <code>target::Bool</code>.</li></ul></li></ol><h3 id="Cross-validation-(match-MBR/regular-scoring)"><a class="docs-heading-anchor" href="#Cross-validation-(match-MBR/regular-scoring)">Cross-validation (match MBR/regular scoring)</a><a id="Cross-validation-(match-MBR/regular-scoring)-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-validation-(match-MBR/regular-scoring)" title="Permalink"></a></h3><ul><li>Use Pioneer’s existing CV folds supplied by the spectral library: <code>getCvFold(precursors, precursor_idx)</code> (from <code>StandardLibraryPrecursors.pid_to_cv_fold</code>).</li><li>This mapping ensures all products of a given peptide (and all precursors belonging to the same protein group) end up in the same CV fold, consistent with MBR and regular scoring.</li><li>Typical folds are two values (0 and 1). Perform CV by holding out one fold and training on the remaining fold(s), rotating across all unique fold values.</li></ul><h3 id="Model-choice"><a class="docs-heading-anchor" href="#Model-choice">Model choice</a><a id="Model-choice-1"></a><a class="docs-heading-anchor-permalink" href="#Model-choice" title="Permalink"></a></h3><ul><li>LightGBM only (binary logistic objective with probability output). No XGBoost.</li></ul><h3 id="Hyperparameters-(match-MBR-scoring;-not-user-configured)"><a class="docs-heading-anchor" href="#Hyperparameters-(match-MBR-scoring;-not-user-configured)">Hyperparameters (match MBR scoring; not user-configured)</a><a id="Hyperparameters-(match-MBR-scoring;-not-user-configured)-1"></a><a class="docs-heading-anchor-permalink" href="#Hyperparameters-(match-MBR-scoring;-not-user-configured)" title="Permalink"></a></h3><ul><li>num_leaves: 63</li><li>max_depth: 10</li><li>learning_rate: 0.15</li><li>min<em>data</em>in_leaf: 1</li><li>feature_fraction: 0.5</li><li>bagging<em>fraction: 0.5 (with `bagging</em>freq = 1`)</li><li>min<em>gain</em>to_split: 0.0</li><li>num_iterations: 200 (aligned with the final LightGBM round used in MBR)</li><li>objective: binary (logistic), outputs calibrated probabilities</li><li>metrics tracked: AUC, binary logloss (diagnostics only)</li></ul><h3 id="Calibration"><a class="docs-heading-anchor" href="#Calibration">Calibration</a><a id="Calibration-1"></a><a class="docs-heading-anchor-permalink" href="#Calibration" title="Permalink"></a></h3><ul><li>Tree outputs are calibrated probabilities under logistic loss; optionally add isotonic calibration on out‑of‑fold predictions if needed. Start without extra calibration for simplicity.</li></ul><h3 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h3><ul><li>Report CV AUC and logloss.</li><li>Compare ROC/AUC of model vs. baseline <code>logodds_baseline</code> to justify replacement.</li><li>Sanity check distributions on held‑out fold.</li></ul><h2 id="Integration-Points"><a class="docs-heading-anchor" href="#Integration-Points">Integration Points</a><a id="Integration-Points-1"></a><a class="docs-heading-anchor-permalink" href="#Integration-Points" title="Permalink"></a></h2><p>Target file: <code>src/Routines/SearchDIA/SearchMethods/ScoringSearch/ScoringSearch.jl</code></p><ol><li>After current computation of per‑run <code>:prec_prob</code> and before the current <code>:global_prob</code> calculation:<ul><li>Build the per‑precursor dictionary of <code>GlobalPrecFeatures{N}</code>.</li><li>Train the model with CV and generate out‑of‑fold predictions for training diagnostics.</li><li>Fit final model on all data.</li></ul></li><li>Generate per‑precursor predictions <code>global_prob_pred[precursor_idx]</code> using LightGBM trained with the library‑defined folds.</li><li>Assign <code>:global_prob = global_prob_pred[precursor_idx]</code> to every row of <code>merged_df</code> by a single vectorized lookup or join.</li><li>Continue existing pipeline steps that consume <code>:global_prob</code> (e.g., global q‑value spline and filtering).</li></ol><h2 id="Configuration"><a class="docs-heading-anchor" href="#Configuration">Configuration</a><a id="Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Configuration" title="Permalink"></a></h2><ul><li>Keep the feature behind a simple enable flag (e.g., <code>optimization.machine_learning.global_prob_model.enabled</code>; default false) and a small set of feature controls only (e.g., <code>top_n</code>, <code>prec_prob_threshold</code>).</li><li>Do not expose model hyperparameters in JSON; they are fixed to the MBR settings listed above.</li></ul><h2 id="File/Type-Additions"><a class="docs-heading-anchor" href="#File/Type-Additions">File/Type Additions</a><a id="File/Type-Additions-1"></a><a class="docs-heading-anchor-permalink" href="#File/Type-Additions" title="Permalink"></a></h2><ul><li>New type: <code>GlobalPrecFeatures{N}</code> in an appropriate module (e.g., <code>src/structs/GlobalProb.jl</code>).</li><li>New utils: <code>build_global_prec_features(merged_df; top_n, prob_thresh, n_runs)</code> to compute the dictionary and DataFrame.</li><li>New ML helper: <code>train_global_prob_model(df, folds, params)</code> returning fitted model and OOF predictions.</li><li>New scorer: <code>predict_global_prob(model, feature_df) -&gt; Dict{UInt32, Float32}</code> mapping <code>precursor_idx</code> to probability.</li></ul><h2 id="Pseudocode-Sketch"><a class="docs-heading-anchor" href="#Pseudocode-Sketch">Pseudocode Sketch</a><a id="Pseudocode-Sketch-1"></a><a class="docs-heading-anchor-permalink" href="#Pseudocode-Sketch" title="Permalink"></a></h2><pre><code class="language-julia hljs"># After prec_prob computed per (precursor_idx, ms_file_idx)
merged_df = DataFrame(Arrow.Table(merged_scores_path))
n_runs = length(getFilePaths(getMSData(ctx)))

dict = Dict{UInt32, GlobalPrecFeatures{TOPN}}()
labels = Dict{UInt32, Bool}()
for g in groupby(merged_df, :precursor_idx)
    pid = first(g.precursor_idx)
    probs = sort!(Float32.(g.prec_prob); rev=true)
    top_scores = ntuple(i -&gt; i &lt;= length(probs) ? probs[i] : 0.0f0, TOPN)
    meanp = mean(probs); maxp = maximum(probs); minp = minimum(probs)
    stdp = length(probs) &gt; 1 ? std(probs) : 0f0
    iqrp = length(probs) &gt;= 4 ? (quantile(probs, 0.75f0) - quantile(probs, 0.25f0)) : 0f0
    nabove = count(&gt;=(prob_thresh), probs)
    del12 = (top_scores[1] - top_scores[2])
    del23 = (top_scores[2] - top_scores[3])
    lodds = logodds(probs, floor(Int, sqrt(n_runs)))  # baseline
    dict[pid] = GlobalPrecFeatures{TOPN}(
        top_scores,
        (meanp, maxp, minp, stdp, iqrp),
        (length(probs), n_runs, nabove),
        (del12, del23),
        lodds,
    )
    labels[pid] = library_is_target(pid)
end

feat_df = features_to_dataframe(dict, labels)  # 1 row per pid
folds = [getCvFold(getPrecursors(getSpecLib(ctx)), pid) for pid in feat_df.precursor_idx]
model, oof_pred = train_global_prob_model(
    feat_df,
    folds;
    num_iterations=200,
    learning_rate=0.15,
    num_leaves=63,
    max_depth=10,
    feature_fraction=0.5,
    bagging_fraction=0.5,
    min_data_in_leaf=1,
    min_gain_to_split=0.0,
)
global_prob_map = predict_global_prob(model, feat_df)

# Assign back to merged_df
merged_df.global_prob = [global_prob_map[pid] for pid in merged_df.precursor_idx]</code></pre><h2 id="Outputs-and-Artifacts"><a class="docs-heading-anchor" href="#Outputs-and-Artifacts">Outputs and Artifacts</a><a id="Outputs-and-Artifacts-1"></a><a class="docs-heading-anchor-permalink" href="#Outputs-and-Artifacts" title="Permalink"></a></h2><ul><li>Save training diagnostics to results folder: CV metrics, ROC curves, feature importances, and OOF prediction histogram.</li><li>Optionally, persist the fitted model parameters (JSON) for reproducibility.</li></ul><h2 id="Testing-Plan"><a class="docs-heading-anchor" href="#Testing-Plan">Testing Plan</a><a id="Testing-Plan-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-Plan" title="Permalink"></a></h2><ul><li>Unit tests for feature builder: correct top‑N padding, stats, counts, baseline.</li><li>Small synthetic dataset: verify CV split stability and no leakage across groups.</li><li>Integration test: run ScoringSearch on test data with the feature flag enabled; check that <code>:global_prob</code> is replaced and that q‑value distributions remain sensible.</li><li>Regression: compare target/decoy separation AUC vs. baseline log‑odds on the same data.</li></ul><h2 id="Risks-and-Mitigations"><a class="docs-heading-anchor" href="#Risks-and-Mitigations">Risks and Mitigations</a><a id="Risks-and-Mitigations-1"></a><a class="docs-heading-anchor-permalink" href="#Risks-and-Mitigations" title="Permalink"></a></h2><ul><li>Leakage across folds via related precursors: Use protein or peptide grouping to partition folds.</li><li>Class imbalance: Use class weights or scale<em>pos</em>weight; also track <code>fdr_scale_factor</code> for consistency with q‑value estimation.</li><li>Dependency sprawl: Use LightGBM only (already present); no new dependencies.</li><li>Overfitting on small datasets: Use early stopping, CV monitoring, and keep feature set compact.</li></ul><h2 id="Rollout"><a class="docs-heading-anchor" href="#Rollout">Rollout</a><a id="Rollout-1"></a><a class="docs-heading-anchor-permalink" href="#Rollout" title="Permalink"></a></h2><ol><li>Implement behind <code>optimization.machine_learning.global_prob_model.enabled</code> (default false).</li><li>Land feature with comprehensive tests and docs; keep old log‑odds as fallback.</li><li>Evaluate on public and internal datasets; if improvements are consistent, flip default.</li></ol></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 14 October 2025 16:42">Tuesday 14 October 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
