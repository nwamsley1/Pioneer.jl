var documenterSearchIndex = {"docs":
[{"location":"development/ci_cd_workflow/#CI/CD-Workflow-Overview","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"This document outlines the GitHub Actions workflows that power continuous integration and delivery for Pioneer.jl.","category":"page"},{"location":"development/ci_cd_workflow/#Workflows","page":"CI/CD Workflow Overview","title":"Workflows","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"The repository uses several workflows:","category":"page"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"tests.yml – run the test suite on Ubuntu\ndocs.yml – build docs for pull requests, tags, and manual runs; deploy docs from main and v* tags\nbuild_app_linux.yml, build_app_macos.yml, build_app_windows.yml – build and package applications; reusable via workflow_call\nrelease.yml – orchestrate cross-platform builds and publish GitHub releases for tags\nCompatHelper.yml – update package compatibility constraints (scheduled daily)\nTagBot.yml – tag releases and interact with the Julia package registry\nregistrator.yml – run Registrator to open a registration PR in the Julia General registry","category":"page"},{"location":"development/ci_cd_workflow/#Event-Matrix","page":"CI/CD Workflow Overview","title":"Event Matrix","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"Event Condition Tests Build Docs Deploy Docs Compile Release Purpose\npush (tag) v*.*.* :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: Release new version\npush main/develop :whitecheckmark: :whitecheckmark: if develop :whitecheckmark: :x: Merge or hotfix\npull request main/develop :whitecheckmark: :whitecheckmark: :x: :whitecheckmark: :x: Completed feature\npull request not main/develop :whitecheckmark: :whitecheckmark: :x: :x: :x: WIP feature\npush not main/develop :whitecheckmark: :whitecheckmark: :x: :x: :x: WIP feature","category":"page"},{"location":"development/ci_cd_workflow/#Manual-Workflow-Dispatches","page":"CI/CD Workflow Overview","title":"Manual Workflow Dispatches","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"Manual dispatches allow running workflows on demand.","category":"page"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"Workflow Condition Tests Build Docs Deploy Docs Compile Release Purpose\nregistrator.yml Julia registration success :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: Release new version\nrelease.yml v*.*.* :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: GitHub release\ntests.yml, docs.yml, build_app_* v*.*.* :whitecheckmark: :whitecheckmark: :whitecheckmark: :whitecheckmark: :x: Manually rerun or troubleshoot\ntests.yml, docs.yml, build_app_* no tag :whitecheckmark: :whitecheckmark: develop :whitecheckmark: :x: Test/build dev version","category":"page"},{"location":"development/ci_cd_workflow/#Release-Process","page":"CI/CD Workflow Overview","title":"Release Process","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"To cut a new release:","category":"page"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"Update the version number in Project.toml.\nMerge that change into the main branch.\nManually trigger registrator.yml, which runs JuliaRegistries/Registrator@v1 to open or update a registration pull request in the Julia General registry.\nAfter the registry PR is merged, TagBot creates a tag and GitHub release.\nThe tag triggers the docs workflow to build and deploy versioned documentation and the release.yml workflow. The release workflow calls the platform build workflows (build_app_linux.yml, build_app_macos.yml, build_app_windows.yml) to compile the application and attach the artifacts to the GitHub release.","category":"page"},{"location":"development/ci_cd_workflow/#Pre-releases","page":"CI/CD Workflow Overview","title":"Pre-releases","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"Tags containing a hyphen (e.g., v1.2.0-rc1) are treated as pre-releases.\nTagBot marks such tags as GitHub prereleases.\nBuild workflows still run, and packaging steps strip pre-release/build metadata when generating Windows installers.","category":"page"},{"location":"development/ci_cd_workflow/#Additional-Notes","page":"CI/CD Workflow Overview","title":"Additional Notes","text":"","category":"section"},{"location":"development/ci_cd_workflow/","page":"CI/CD Workflow Overview","title":"CI/CD Workflow Overview","text":"The docs workflow builds documentation for all pull requests, tags, and manual runs, but only deploys to gh-pages when invoked on main, develop, or a v* tag.\nTag-based releases (v*.*.*) trigger tests, docs, builds, publishing of GitHub releases, and versioned documentation.\nCompatHelper runs nightly to propose dependency updates; TagBot publishes tags to the Julia General registry.\nDocumentation is published for stable, semantic version tags (v#.#.#), and dev.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"One-to-One Target–Decoy Pairing With Decoy Cloning","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Goal","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Guarantee that every pairid identifies exactly one target and one decoy across PSMs, even when multiple targets map to a single decoy. Avoid unintended “target A vs target B” grouping in later stages (e.g., summarizeprecursors!, applymbrfilter!). Persist pair_id to PSM files so MBR filtering operates on correct 1:1 pairs.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Problems To Solve","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Many-to-one: Multiple targets (A, B, …) may need the same decoy C. If all rows A, B, C share the same pair_id, the “pair” can accidentally include A–B together.\nDownstream grouping: summarizeprecursors! and applymbrfilter! group by pairid. If pair_id is not 1:1, we get incorrect within-run comparisons and false transfers.\nTraining leakage: Cloned decoy rows should not distort training distributions, but they must exist later for correct pairing and MBR grouping.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Design Overview","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Decoy cloning: For each additional target that is assigned to decoy C beyond the first, create a cloned decoy row (same chromatogram features) and assign a new unique pairid for the target–clone pair. The original decoy keeps a distinct pairid with its first target.\n1:1 invariant: After cloning, each pair_id appears in exactly two roles: one target and one decoy (canonical or clone).\nExclude clones from training: Cloned decoy rows are not used when fitting models but must be present for inference and written out to per-run PSM files after scoring.\nStratified pairing: Create target–decoy assignments within 10×10 bins in (prec_mz × iRT) to preserve similarity of pairs. Use global fallback bins when strata are sparse.\nDeterministic: Use a fixed RNG seed for reproducibility.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Data Model Additions","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"New columns on the PSM DataFrame (in-memory only; all exported to files except those marked ephemeral):\n:pair_id::UInt32 – unique 1:1 identifier for target–decoy pairs (persisted to Arrow files).\n:pair_role::UInt8 – 0=target, 1=decoy (canonical), 2=decoy_clone (optional; can derive from :target and clone flag).\n:pair_clone_of::Union{Missing,UInt32} – precursor_idx of the canonical decoy for clones; missing for targets and canonical decoys.\n:pair_training_mask::Bool – false for clones to exclude them from training sets (ephemeral).","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Where To Implement","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"File: src/utils/ML/percolatorSortOf.jl\nFunction entry: sort_of_percolator_in_memory!\nTiming: Very start, before sort!(psms, [:pair_id, :isotopes_captured])","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Algorithm (Pair Generation)","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Build precursor-level table","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Collapse PSMs to unique precursor_idx, keeping first target::Bool, prec_mz::Float32, and iRT column. iRT precedence: :irt_pred > :irt_obs; if both missing, use a single iRT bin.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Create 10×10 bins (prec_mz × iRT)","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Compute quantile-based edges for prec_mz and chosen iRT (0:0.1:1.0). Deduplicate and enforce strictly increasing edges; otherwise fallback to even-width LinRange(min,max,11).\nAssign each precursor to a (binmz, binirt) stratum via searchsortedlast and clamp to [1,10]. If iRT missing, use 10×1 bins.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Stratified target–decoy assignment per stratum","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"For each stratum s:\ntargets_s = shuffle(target precursors in s)\ndecoys_s = shuffle(decoy precursors in s)\nIf one side is empty locally, use a globally-shuffled pool from the full dataset for that side.\nDetermine which side is smaller:\nIf length(decoys_s) < length(targets_s): assign each decoy to multiple targets (decoy-reuse). Maintain mapping assignments_decoy[d] => Vector{targets}.\nIf length(targets_s) < length(decoys_s): assign each target to multiple decoys (target-reuse). Maintain mapping assignments_target[t] => Vector{decoys}.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Construct pair_id and clones (no-unpaired invariant)","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"We ensure every precursor participates in a 1:1 pair_id unless one side is truly absent (no targets or no decoys even after global fallback). Choose cloning based on which side is larger in the stratum:\nDecoys fewer than targets (decoy-reuse): For each decoy d with assigned targets [t1, t2, …]:\nFirst target t1: pair_id = next_id(); assign to t1 and canonical d.\nEach additional target ti (i ≥ 2): clone d’s PSM rows per run; set pair_id = next_id(), pair_clone_of = d, pair_training_mask = false.\nTargets fewer than decoys (target-reuse): For each target t with assigned decoys [d1, d2, …]:\nFirst decoy d1: pair_id = next_id(); assign to canonical t and d1.\nEach additional decoy dj (j ≥ 2): clone t’s PSM rows per run; set pair_id = next_id(), pair_clone_of = t, pair_training_mask = false.\nCanonical (non-clone) rows always have pair_training_mask = true.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Update the full PSM DataFrame","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Overwrite/create :pair_id with the newly assigned values for all rows (canonical + clones + targets).\nAdd :pair_clone_of and :pair_training_mask as above.\nPreserve original ordering, or re-sort by [:pair_id, :isotopes_captured] as today.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Training, Prediction, and Write-Back","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Exclude clones when selecting training rows","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"In get_training_data_for_iteration!, filter by pair_training_mask (keep true rows only). This avoids inflating decoy counts.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Inference on clones","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Keep clones in the test/inference set (fold assignment unchanged). They receive model probabilities directly. Alternatively (optimization), copy predictions from their canonical decoy; initially, compute directly for simplicity and correctness.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Persist to Arrow files","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"write_scored_psms_to_files! writes out the PSMs including the cloned decoy rows and the regenerated pair_id. Keep :pair_id and drop only vector columns as before.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Downstream Changes","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"applymbrfilter! requires :pair_id","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Remove the fallback that re-derives pair_id from the library.\nIf :pair_id is missing in mergeddf, throw an error with a clear message: the file must be produced by the new pipeline that regenerates pairid.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"summarize_precursors! grouping remains the same","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"With 1:1 pair_id, within-run target–decoy logic is correct. No further changes required aside from previously added robustness to use :mbr_prob or :prob.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"applymbrfilter! adjustments (with 1:1 pair_id)","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Preconditions:\nmergeddf contains `:pairid::UInt32,:prob::Float32,:target::Bool,:decoy::Bool,:MBRtransfercandidate::Bool, and:msfileidx`.\nEach :pair_id denotes exactly one target and one decoy per run (thanks to cloning during pairing).\nRemove library-derived pair_id code:\nDelete any attempt to recompute :pair_id from the spectral library (e.g., via getPairId).\nInsert a hard check: if :pair_id ∉ propertynames(mergeddf) → `error(\"pairid missing; regenerate pairs before ScoringSearch\")`.\nCandidate set and non-candidates:\ncandidate_mask = merged_df.MBR_transfer_candidate.\nCompute trace q-values only on non-candidates with library FDR scaling for diagnostics: get_qvalues!(merged_df.prob[.!candidate_mask], merged_df.target[.!candidate_mask], trace_qval[.!candidate_mask]).\nWithin-run target–decoy dominance:\nGroup candidates by [:ms_file_idx, :pair_id].\nFor each group, identify best target and best decoy (there should be at most one of each). If either is missing, skip the group with a warning counter.\nIf best_decoy_prob ≥ best_target_prob, mark the target row in this group as a transfer decoy (bad). Always mark decoy rows as transfer decoys, since they are the reference negatives.\nBuild bad-mask and threshold:\nbad_mask = candidate_mask .& (merged_df.decoy .| target_marked_as_bad).\nCompute τ with get_ftr_threshold(merged_df.prob, merged_df.target, bad_mask, α; mask=candidate_mask), where α = params.max_MBR_false_transfer_rate (or α' if alpha-scaling experiment is enabled).\nClamp candidate probabilities:\nmerged_df._filtered_prob = ifelse.(candidate_mask .& (merged_df.prob .< τ), 0f0, merged_df.prob) and return :_filtered_prob.\nLogging and diagnostics:\nCount candidate groups lacking a target or decoy; log as potential data issues.\nLog (α, τ, #candidates, #transfer_decoys) and a small sample of group-level decisions for QA.\nInteraction with stratified FTR experiments:\nIf later stratifying FTR calibration, reuse :pair_id structure but compute τ within strata (e.g., by MBR_num_runs or donor-count), applying the above dominance rule inside each stratum.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Validation & Diagnostics","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Before/after stats:\nUnique pair_id count; distribution of group sizes (expect mostly 2, some 1).\nFraction of decoys cloned (and average clone count).\nPer-stratum pairing coverage; number of strata using global fallback.\nCorrectness checks:\nFor each pair_id, ensure at most one target and at most one decoy row per run.\nOn a small dataset, assert the 1:1 property holds globally.\nLogging:\nSeed, bins used, stratum sizes, counts of clones, any fallbacks triggered.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Performance Considerations","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Cloning increases row count by up to the number of extra target assignments per decoy. In typical DIA datasets, this should be a moderate multiplier. Monitor memory and optionally gate on dataset size.\nIf needed later, add a config toggle to disable cloning (fallback to original behavior) for memory-constrained runs.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Rollout Steps & Commits","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Commit checkpoint (baseline):","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Commit current state before changes as chore(MBR): checkpoint before 1:1 pairing work.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Implement pairing + cloning + training mask + persistence","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Add regenerate_pair_ids! helper and call at top of sort_of_percolator_in_memory!.\nModify get_training_data_for_iteration! to respect pair_training_mask.\nEnsure write_scored_psms_to_files! persists :pair_id (and optionally :pair_clone_of for debugging).","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Update applymbrfilter!","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Remove library-derived pair_id fallback.\nError if :pair_id missing in merged_df.","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Commit implementation:","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"feat(MBR): 1:1 target–decoy pairing with decoy cloning and persisted pair_id","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Verification run","category":"page"},{"location":"advanced/mbr_pairing_strategy/","page":"-","title":"-","text":"Run a representative dataset; check logs and pairing stats; verify applymbrfilter! sees pair_id and completes.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Simpler Pairing Strategy — Critique And Alternative Plan","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"This document proposes a simpler approach to pairing for MBR that achieves the same downstream effect without cloning rows or regenerating pair_id at scoring time. It also critiques the prior “simple” pairing plan (not in repo here) that introduced additional complexity.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Critique Of The Prior Plan","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Complexity and memory cost: Cloning decoy (or target) rows to enforce 1:1 pair_id multiplies the PSM table size and introduces quadratic‑like costs if clones are appended iteratively.\nMoving targets: Re‑pairing at scoring time overrides a stable library invariant. The spectral library already encodes target/decoy pairing (e.g., pair_id) upstream; redefining pairs late in the pipeline makes debugging harder and risks drift between runs.\nCoupling to downstream logic: Enforcing 1:1 at the row level only to satisfy a specific grouping in apply_mbr_filter! creates tight coupling. A better approach is to compute dominance flags directly and keep apply_mbr_filter! agnostic to strict 1:1 groups.\nCV‑fold nuances: Pairing within cv_fold and bins is brittle. The scorer already computes robust MBR features per run; the pairing step is redundant if we express “decoy outranks target” with per‑row flags.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Goal","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Preserve the practical outcome (flag and control risky transfers) without cloning or regenerating pair_id.\nReduce CPU/memory use, avoid write‑amplification, and keep the data model stable.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Key Observation","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"The MBR filter does not inherently require 1:1 pair groups if we provide per‑row dominance flags. Earlier we already compute:\nMBR_max_pair_prob and MBR_is_best_decoy in summarize_precursors!.\nA candidate mask in the scorer.\napply_mbr_filter! can operate on those signals without grouping.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Proposed Simpler Design","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Do not regenerate pair_id at scoring time","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Keep the library’s pair_id (if present) as a passive attribute. Don’t modify it or rely on it for strict 1:1.\nIf pair_id is missing in some PSMs, that’s okay — the steps below don’t depend on it.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Fix candidate labeling in the in‑memory scorer","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"File: src/utils/ML/percolatorSortOf.jl\nFunction: sort_of_percolator_in_memory!\nReplace the probability‑based pass mask with q‑value based logic (mirroring update_mbr_probs!):","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"qvals_prev = similar(nonMBR_estimates)\nget_qvalues!(nonMBR_estimates, psms.target, qvals_prev)\npass_mask = (qvals_prev .<= max_q_value_lightgbm_rescore) .& psms.target\nprob_thresh = any(pass_mask) ? minimum(nonMBR_estimates[pass_mask]) : typemax(Float32)\npsms[!, :MBR_transfer_candidate] .= (qvals_prev .> max_q_value_lightgbm_rescore) .&\n                                    (psms.MBR_max_pair_prob .>= prob_thresh)","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Compute per‑row dominance without cloning","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"summarize_precursors! already determines, per run, a “best other run” via MBR_max_pair_prob and flags MBR_is_best_decoy.\nIf we want a single, direct dominance flag for filtering, compute it after final probabilities with a single grouped combine over candidates only (no cloning, no pair regeneration):","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"agg = combine(groupby(view(psms, psms.MBR_transfer_candidate, :),\n                      [:ms_file_idx, :pair_id, :target]),\n              :prob => maximum => :max_prob)\ntgt = rename!(agg[agg.target .== true, [:ms_file_idx, :pair_id, :max_prob]], :max_prob => :tmax)\ndcy = rename!(agg[agg.target .== false, [:ms_file_idx, :pair_id, :max_prob]], :max_prob => :dmax)\npairmax = outerjoin(tgt, dcy, on=[:ms_file_idx, :pair_id])\npairmax[!, :MBR_paired_decoy_higher] = coalesce.(pairmax.dmax, -Inf32) .> coalesce.(pairmax.tmax, -Inf32)\npsms = leftjoin(psms, pairmax[:, [:ms_file_idx, :pair_id, :MBR_paired_decoy_higher]], on=[:ms_file_idx, :pair_id])\npsms[!, :MBR_paired_decoy_higher] = ifelse.(psms.target, coalesce.(psms.MBR_paired_decoy_higher, false), false)","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Notes:","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"This is optional if MBR_is_best_decoy is already reliable for filtering.\nIt avoids cloning and only touches the minimal set of rows.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Keep apply_mbr_filter! simple","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"File: src/Routines/SearchDIA/SearchMethods/ScoringSearch/scoring_interface.jl\nFunction: apply_mbr_filter!\nUse the candidate mask and a simple bad‑transfer predicate (original form):","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"candidate_mask = merged_df.MBR_transfer_candidate\nis_bad_transfer = candidate_mask .& (\n    (merged_df.target .& coalesce.(merged_df.MBR_is_best_decoy, false)) .|\n    merged_df.decoy\n)\nτ = get_ftr_threshold(merged_df.prob, merged_df.target,\n                      is_bad_transfer, params.max_MBR_false_transfer_rate;\n                      mask=candidate_mask)\nmerged_df._filtered_prob = ifelse.(candidate_mask .& (merged_df.prob .< τ), 0f0, merged_df.prob)","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"This design works whether pair_id exists or not and does not require strict 1:1 pairing.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Benefits","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"No cloning or re‑pairing: Lower memory/CPU, simpler control flow, smaller risk of regression.\nStable semantics: Relies on per‑row flags and q‑value logic already present.\nDecoupled filter: apply_mbr_filter! needs only a candidate mask and a bad‑transfer mask — no grouping gymnastics.","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Implementation Notes","category":"page"},{"location":"advanced/simple_pairing_plan_alt/","page":"-","title":"-","text":"Instrumentation: Log candidate counts and prob_thresh after labeling to confirm realistic set sizes.\nOut‑of‑memory parity: The OOM path already uses correct q‑value‑based labeling (update_mbr_probs!). With this plan, both paths are aligned semantically.","category":"page"},{"location":"supplemental_methods/CLAUDE/#Supplemental-Methods-Documentation","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/#Overview","page":"Supplemental Methods Documentation","title":"Overview","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"all_methods.tex is a comprehensive LaTeX draft of supplemental methods for a manuscript describing Pioneer and Altimeter, a DIA (Data-Independent Acquisition) proteomics analysis software suite optimized for narrow isolation windows.","category":"page"},{"location":"supplemental_methods/CLAUDE/#Document-Structure","page":"Supplemental Methods Documentation","title":"Document Structure","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"The document is formatted for Nature journal submission (sn-nature document class) with ~1079 lines and includes detailed mathematical formulations for all algorithms.","category":"page"},{"location":"supplemental_methods/CLAUDE/#Main-Sections","page":"Supplemental Methods Documentation","title":"Main Sections","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Altimeter Training Data & Model (Lines 51-149)\nTraining dataset from ProteomeTools project (PRIDE archives)\nDatabase searching with sage v0.14.7\nSpectrum filtering and quality control\nFragment ion annotation and deisotoping\nNCE (normalized collision energy) alignment\nTransformer-based model architecture (11.8M parameters)\nCubic B-spline coefficients for fragment intensity prediction\nKoina framework deployment with 4 inference variants\nPioneer - File Conversion (Lines 168-170)\nCross-platform MS file conversion to Apache Arrow IPC format\nSupports Thermo .raw and .mzML files\nPioneerConverter tool (GitHub: nwamsley1/PioneerConverter)\nSpectral Library Generation (Lines 175-184)\nFASTA protein sequence digestion\nTarget-decoy sequence generation (reverse/shuffle)\nEntrapment sequence integration for FDR calibration\nProtein group definition (mathematical formulation)\nIntensity-Aware Fragment Index Search (Lines 188-283)\nModified fragment-index algorithm accounting for library intensities\nHierarchical bin structure (retention time → fragment m/z)\nScore counter data structure\nBinary search-based MS/MS scan queries\nParameter Tuning (Lines 293-381)\nPre-search for run-specific parameter estimation\nRetention time alignment via B-splines\nMass error/tolerance estimation (exponential distributions)\nNCE alignment with piecewise linear model\nFirst Pass Search (Lines 386-526)\nIterative training procedure based on Percolator\nProbit regression model for PSM scoring\nPEP (Posterior Error Probability) estimation via wPAVA\nPSM aggregation across runs\nRefined RT alignment and tolerance estimation\nSpectral Deconvolution (Lines 534-598)\nMatrix representation of library/empirical spectra\nSparse column-major layout for efficiency\nPseudo-Huber loss minimization with non-negativity constraints\nCoordinate descent optimization with Newton-Raphson solver\nHot-start initialization\nTarget-Decoy Model & Match-Between-Runs (Lines 602-860)\nLightGBM models with cross-validation\nIterative training with negative mining\nMBR (Match-Between-Runs) features:\nRV coefficient for chromatographic similarity\nRetention time differences\nIntensity ratios\nCross-run evidence\nFalse Transfer Rate (FTR) filtering\nTwo-stage FDR control (global + experiment-wide q-values)\nProbability aggregation (trace → precursor-run → global)\nChromatogram Quantification (Lines 864-906)\nWhittaker-Henderson smoothing\nApex refinement\nPeak boundary detection via second derivative\nBaseline subtraction\nTrapezoidal integration\nProtein Inference & Quantification (Lines 911-1067)\nParsimony-based inference algorithm\nTwo-phase approach: unique peptides → greedy set cover\nBipartite graph decomposition via DFS\nLightGBM-based protein scoring (optional)\nMaxLFQ algorithm for label-free quantification\nFragment Isotope Correction (Lines 1073-end)\nConditional fragment isotope probabilities\nQuadrupole-filtered precursor isotope distribution\nRe-isotoping of library spectra","category":"page"},{"location":"supplemental_methods/CLAUDE/#Key-Features","page":"Supplemental Methods Documentation","title":"Key Features","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/#Mathematical-Rigor","page":"Supplemental Methods Documentation","title":"Mathematical Rigor","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Formal definitions using set notation\nDetailed algorithm pseudocode (e.g., protein inference algorithm)\nLoss functions, optimization objectives clearly stated\nStatistical models (probit regression, isotonic regression, exponential fits)","category":"page"},{"location":"supplemental_methods/CLAUDE/#Cross-References-to-Code","page":"Supplemental Methods Documentation","title":"Cross-References to Code","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"The document is designed to map to the Julia implementation in Pioneer.jl. Line numbers in the protein inference algorithm (lines 117-380) appear to reference actual source code.","category":"page"},{"location":"supplemental_methods/CLAUDE/#FDR-Control-Strategy","page":"Supplemental Methods Documentation","title":"FDR Control Strategy","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Multi-layered approach:","category":"page"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Spectrum-level (Sage searches)\nPSM-level (Percolator-style iterative training)\nPrecursor-level (two-stage global + experiment-wide)\nProtein-level (target-decoy competition)\nTransfer-level (FTR for match-between-runs)","category":"page"},{"location":"supplemental_methods/CLAUDE/#Novel-Contributions","page":"Supplemental Methods Documentation","title":"Novel Contributions","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Intensity-aware fragment indexing - accounts for predicted intensities in scoring\nNCE alignment - maps vendor collision energies to PROCAL Lumos scale\nConditional fragment isotopes - models isotopes based on quadrupole transmission\nFTR filtering - controls false transfers in match-between-runs\nIntegrated workflow - spectral prediction (Altimeter) → search → quantification (Pioneer)","category":"page"},{"location":"supplemental_methods/CLAUDE/#Technical-Details","page":"Supplemental Methods Documentation","title":"Technical Details","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/#Authors","page":"Supplemental Methods Documentation","title":"Authors","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Nathan T. Wamsley (lead)\nEmily M. Wilkerson\nBen Major\nDennis Goldfarb (corresponding)","category":"page"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Washington University School of Medicine","category":"page"},{"location":"supplemental_methods/CLAUDE/#Software-Stack","page":"Supplemental Methods Documentation","title":"Software Stack","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Altimeter: PyTorch transformer model (UniSpec architecture)\nPioneer: Julia implementation\nSearch: Sage v0.14.7\nML Models: LightGBM, probit regression\nQuantification: MaxLFQ\nFile I/O: Apache Arrow IPC format","category":"page"},{"location":"supplemental_methods/CLAUDE/#Data-Sources","page":"Supplemental Methods Documentation","title":"Data Sources","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"ProteomeTools PRIDE datasets: PXD021013, PXD010595, PXD004732, PXD006832\nUniProt human reference proteome (2024-06-04)","category":"page"},{"location":"supplemental_methods/CLAUDE/#Implementation-Notes","page":"Supplemental Methods Documentation","title":"Implementation Notes","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"When working with Pioneer.jl code:","category":"page"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Protein inference logic is in src/utils/proteinInference.jl\nLine numbers in the algorithm pseudocode (lines 117-380) map to source\nThe two-phase approach (unique peptides first, then greedy set cover) is the core logic\nConnected components are discovered via DFS before inference\nQuantification flags (use_for_quant) distinguish unique vs. shared peptides","category":"page"},{"location":"supplemental_methods/CLAUDE/#Document-Status","page":"Supplemental Methods Documentation","title":"Document Status","text":"","category":"section"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"DRAFT - This is supplemental methods for an in-preparation manuscript. Expect:","category":"page"},{"location":"supplemental_methods/CLAUDE/","page":"Supplemental Methods Documentation","title":"Supplemental Methods Documentation","text":"Missing cross-references (marked with \\ref{REFHERE}, \\ref{ref here})\nPlaceholder citations (e.g., \"LightGBM paper...\")\nTODO markers in red text (e.g., \\textcolor{red}{...})\nPotential formatting/equation adjustments before submission","category":"page"},{"location":"api/core/#api-reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"user_guide/parameters/#Parameter-Configuration","page":"Parameter Configuration","title":"Parameter Configuration","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Pioneer.jl uses JSON configuration files to control analysis. This guide explains the parameters for both SearchDIA and BuildSpecLib functions.","category":"page"},{"location":"user_guide/parameters/#SearchDIA-Configuration","page":"Parameter Configuration","title":"SearchDIA Configuration","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Pioneer.jl uses JSON configuration files to control analysis. This guide explains the parameters for both SearchDIA and BuildSpecLib functions.","category":"page"},{"location":"user_guide/parameters/#SearchDIA-Configuration-2","page":"Parameter Configuration","title":"SearchDIA Configuration","text":"","category":"section"},{"location":"user_guide/parameters/#Frequently-Modified-Parameters","page":"Parameter Configuration","title":"Frequently Modified Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Most parameters should not be changed, but the following may need adjustement. ","category":"page"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"first_search.fragment_settings.min_score: The minimum score determines which fragments must match in the fragment-index search in order for the precursor to pass. Each precursor is awarded a score based on which fragments match the spectrum. The score assigned to each fragment depends on its intensity rank. The default scheme is 8,4,4,2,2,1,1. That is, if the 1st, 3rd, and 7th ranking fragments matched the spectrum, the precursor would be awarded a score of 8+4+1=13. If all 7 of the fragments matched, the precursor would be awarded a score of 22. For normal instrument settings on an Orbitrap or Astral mass analyzer, the mass tolerance is about +/- 5-15 ppm and 15 is a reasonable default score threshold. However, for instruments with less mass accuracy (Sciex ZenoTOF 7600 or different Orbitrap scan settings), the score threshold may need to be set higher, perhaps to 20. It may be worthwile to test different values when searching data from a new instrument or sample type. In order to pass the first search, a precursor need only pass the threshold and score sufficiently well in at least one of the MS data files.\nfirst_search.fragment_settings.max_rank: Search against only the n'th most abundant fragment for each precursor. Including more fragments can improve performance but increase memory consumption, and the search could take longer. From experience, there are diminishing returns after 25-50 fragments. \nquant_search.fragment_settings.max_rank: See above \nquant_search.fragment_settings.n_isotopes: If searching with non-Altimeter libraries (not recommended), such as Prosit or UniSpec, this should be set to 1 as the second fragment isotopes will not be calculated accurately.\nacquisition.nce: This is the initial guess for the normalized collision energy that will best align the Altimeter Library with the empirical data. Altimeter values should agree with those from Thermo Instruments manufactured in Bremen Germany. If upon inspection of the quality control plots the initial guess is far from the estimated value, it might be possible to improve search results slightly by re-searching with a better initial guess.\nacquisition.quad_transmission.fit_from_data: Estimate the quad transmission function from the data. Otherwise defaults to symmetric, smooth function. \noptimization.machine_learning.max_samples: This is the maximum number of PSMs to use for training the LightGBM model. These PSMs need to comfortably fit in memory in addition to the spectral library. As a rule of thumb, 7M rows is about 1GB. At the default maximum of 50M rows, the PSMs table will consume 7GB of memory.\nDuring LightGBM training, any missing feature values are replaced with the column median. If a column is entirely missing, the values are filled with zero of the appropriate type.\nglobal.isotope_settings.combine_traces: Some precursors may be split accross different acquisition windows. Pioneer refers to these as seperate isotope traces. When set to true, Pioneer does not distinguish between a precursor's isotope traces. They are combined for scoring and quantitation. With a clever acquisition scheme this can increase the number of data points accross chromatographic peaks. This is recomended only for acquisition windows 2-4 m/z. It should also be combined with aquisition.quad_transmission.fit_from_data = true. ","category":"page"},{"location":"user_guide/parameters/#Global-Parameters","page":"Parameter Configuration","title":"Global Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nisotope_settings.err_bounds_first_pass [Int, Int] Precursor monoisotope may lie NEUTRON/charge Thompsons (left, right) outside the quadrupole isolation window (default: [1, 0])\nisotope_settings.err_bounds_second_pass [Int, Int] Precursor monoisotope may lie NEUTRON/charge Thompsons (left, right) outside the quadrupole isolation window (default: [3, 1])\nisotope_settings.combine_traces Boolean Whether to combine precursor isotope traces in quantification. Experimental, so set to false (default: false)\nisotope_settings.partial_capture Boolean Whether to estimate the conditional fragment isotope distribution (true) or assume complete transmission the entire precursor isotopic envelope (default: true)\nisotope_settings.min_fraction_transmitted Float Minimum fraction of the precursor isotope distribution that must be isolated for scoring and quantitation (default: 0.25)\nscoring.q_value_threshold Float Global q-value threshold for filtering results. Also controls false transfer rate of MBR (default: 0.01)\nnormalization.n_rt_bins Int Number of retention time bins for quant normalization (default: 100)\nnormalization.spline_n_knots Int Number of knots in quant normalization spline (default: 7)\nmatch_between_runs Boolean Whether to attempt to transfer peptide identifications across runs. Turning this on will add additional features to the LightGBM model (default: true)","category":"page"},{"location":"user_guide/parameters/#Parameter-Tuning-Settings","page":"Parameter Configuration","title":"Parameter Tuning Settings","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nfragment_settings.min_count Int Minimum number of matching fragment ions (default: 7)\nfragment_settings.max_rank Int Maximum rank of fragments to consider (default: 25, means 26th-last most abundant fragments per precursor are filtered out)\nfragment_settings.tol_ppm Float Initial tragment mass tolerance guess in parts per million (default: 20.0, should be set lower for some TOF instruments)\nfragment_settings.min_score Int Minimum fragment-index score threshold for fragment matches (default: 22)\nfragment_settings.min_spectral_contrast Float Minimum cosine simmilarity score (default: 0.9)\nfragment_settings.relative_improvement_threshold Float Minimum relative Scribe score improvement needed to ignore an interferring peak (default: 1.25)\nfragment_settings.min_log2_ratio Float Minimum log2 ratio of matched library fragment intensities to unmatched library fragment intensities (default: 1.5)\nfragment_settings.min_top_n [Int, Int] Minimum number of top N matches - [requirement, denominator]. Default: [3, 3]\nfragment_settings.n_isotopes Int Number of fragment isotopes to consider in matching (default: 1, mono only)\nsearch_settings.min_samples Int Minimum number of samples required for tuning (default: 3500)\nsearch_settings.min_quad_tuning_psms Int Minimum number of psms required for estimating quad transmission (default: 5000)\nsearch_settings.min_quad_tuning_fragments Int Must match at least n fragments to each quad tuning psm (default: 3)\nsearch_settings.max_presearch_iters Int Maximum number of parameter tuning iterations (default: 10)\nsearch_settings.frag_err_quantile Float Quantile for fragment error estimation (default: 0.01)","category":"page"},{"location":"user_guide/parameters/#First-Search-Parameters","page":"Parameter Configuration","title":"First Search Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nfragment_settings.min_count Int Minimum number of matching fragments (default: 4)\nfragment_settings.max_rank Int Maximum fragment rank to consider (default: 50 means 50th-last most abundant fragments per precursor are filtered out)\nfragment_settings.min_score Int Minimum score for fragment matches (default: 15)\nfragment_settings.min_spectral_contrast Float Minimum cosine simmilarity required (default: 0.5)\nfragment_settings.relative_improvement_threshold Float Minimum relative Scribe score improvement needed to ignore an interferring peak (default: 1.25)\nfragment_settings.min_log2_ratio Float Minimum log2 ratio of matched library fragment intensities to unmatched library fragment intensities (default: 0.0, means sum of matched library fragment intensities is equal to the sum of unmatched library fragment intensities for the precursor )\nfragment_settings.min_top_n [Int, Int] Minimum top N matches - [requirement, denominator]. Default: [2, 3]\nfragment_settings.n_isotopes Int Number of isotopes to consider (default: 1)\nscoring_settings.n_train_rounds Int Number of training rounds for scoring model (default: 2)\nscoring_settings.max_iterations Int Maximum iterations for scoring optimization (default: 20)\nscoring_settings.max_q_value_probit_rescore Float Maximum q-value threshold for semi-supervised learning durning probit regression (default: 0.05)\nscoring_settings.max_PEP Int Maximum local FDR threshold for passing the first search (default: 0.9)\nirt_mapping.max_prob_to_impute_irt Int If probability of the psm is less then x in the first-pass search, then impute irt for the precursor with globably determined value from the other runs (default: 0.75)\nirt_mapping.fwhm_nstd Float Number of standard deviations of the fwhm to add to the retention time tolerance (default: 4)\nirt_mapping.irt_nstd Int Number of standard deviations of run-to-run irt tolerance to add to the retention time tolerance (default: 4)","category":"page"},{"location":"user_guide/parameters/#Quantification-Search-Parameters","page":"Parameter Configuration","title":"Quantification Search Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nfragment_settings.min_count Int Minimum fragment count for quantification (default: 3)\nfragment_settings.min_y_count Int Minimum number of y-ions required (default: 2)\nfragment_settings.max_rank Int Maximum fragment rank (default: 255)\nfragment_settings.min_spectral_contrast Float Minimum spectral contrast score (default: 0.0)\nfragment_settings.min_log2_ratio Float Minimum log2 ratio of intensities (default: -1.7)\nfragment_settings.min_top_n [Int, Int] Minimum top N matches - [requirement, denominator]. Default: [2, 3]\nfragment_settings.n_isotopes Int Number of isotopes for quantification (default: 2, include the M1 and M2 isotopes)\nchromatogram.smoothing_strength Float Strength of chromatogram smoothing (default: 1e-6)\nchromatogram.padding Int Number of zeros to pad chromatograms on either side (default: 0)\nchromatogram.max_apex_offset Int Maximum allowed apex offset in #scans where the precursor could have been detected between the second-pass search and re-integration with 1 percent FDR precursors (default: 2)","category":"page"},{"location":"user_guide/parameters/#Acquisition-Parameters","page":"Parameter Configuration","title":"Acquisition Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nnce Int Normalized collision energy initial guess (used in pre-search before NCE tuning) (default: 25)\nquad_transmission.fit_from_data Boolean Whether to fit quadrupole transmission from data (default: false)\nquad_transmission.overhang Float deprecated (default: 0.25)\nquad_transmission.smoothness Float Smoothness parameter for transmission curve. Higher value means more \"box-like\" shape. (default: 5.0)","category":"page"},{"location":"user_guide/parameters/#RT-Alignment-Parameters","page":"Parameter Configuration","title":"RT Alignment Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nsigma_tolerance Int Number of standard deviations for irt tolerance after pre-search (default: 4)\nmin_probability Float Minimum probability for alignment psms in pre-search (default: 0.95)","category":"page"},{"location":"user_guide/parameters/#Optimization-Parameters","page":"Parameter Configuration","title":"Optimization Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\ndeconvolution.lambda Float L2 regularization parameter for deconvolution (default: 0.0 for NoNorm regularization)\ndeconvolution.reg_type String Regularization type: \"none\" (NoNorm), \"l1\" (L1Norm), or \"l2\" (L2Norm) (default: \"none\")\ndeconvolution.huber_delta Float Delta parameter for Huber loss function (default: 300)\ndeconvolution.huber_exp Float Exponent for Huber delta progression (default: 1.5)\ndeconvolution.huber_iters Int Number of Huber outer iterations (default: 15)\ndeconvolution.newton_iters Int Maximum Newton iterations per outer iteration (recommended: 25, default: 50)\ndeconvolution.bisection_iters Int Maximum bisection iterations when Newton fails (recommended: 100, default: 100)\ndeconvolution.outer_iters Int Maximum outer iterations for convergence (recommended: max(1000, n_variables*5), default: 1000)\ndeconvolution.newton_accuracy Float Absolute convergence threshold for Newton method (default: 10)\ndeconvolution.bisection_accuracy Float Absolute convergence threshold for bisection method (default: 10)\ndeconvolution.max_diff Float Relative convergence threshold - maximum relative change in weights between iterations. Also used as relative tolerance for Newton's method (default: 0.01)\nmachine_learning.max_samples Int Maximum number of samples for LightGBM training (default: 5000000)\nmachine_learning.min_trace_prob Float Minimum trace probability threshold (default: 0.75)\nmachine_learning.max_q_value_mbr_itr Float q-value threshold for match-between-runs candidates kept during the iterative training (ITR) stage of LightGBM rescoring (default: 0.20)\nmachine_learning.min_PEP_neg_threshold_itr Float Minimum posterior error probability threshold for reclassifying weak target PSMs as negatives during the ITR stage of LightGBM rescoring (default: 0.90)\nmachine_learning.spline_points Int Number of points for probability spline (default: 500)\nmachine_learning.interpolation_points Int Number of interpolation points (default: 10)","category":"page"},{"location":"user_guide/parameters/#Protein-Inference-Parameters","page":"Parameter Configuration","title":"Protein Inference Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nmin_peptides Int Minimum number of peptides required for a protein group (default: 1)","category":"page"},{"location":"user_guide/parameters/#MaxLFQ-Parameters","page":"Parameter Configuration","title":"MaxLFQ Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nrun_to_run_normalization Boolean Whether to use run-to-run normalized abundances for precursor and protein quantification (default: true)","category":"page"},{"location":"user_guide/parameters/#Output-Parameters","page":"Parameter Configuration","title":"Output Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nwrite_csv Boolean Whether to write results to CSV\nwrite_decoys Boolean Where to quantify and include decoys in the output files\ndelete_temp Boolean Whether to delete temporary files\nplots_per_page Int Number of plots per page in reports (default: 12)","category":"page"},{"location":"user_guide/parameters/#Logging-Parameters","page":"Parameter Configuration","title":"Logging Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\ndebug_console_level Int Verbosity of console debug output (0 disables; higher values include more details).\nmax_message_bytes Int Maximum bytes of a single log message before truncation (default: 4096). Truncation preserves valid UTF-8 and appends a suffix like … [truncated N bytes]. Can be overridden at runtime with PIONEER_MAX_LOG_MSG_BYTES (values clamped to [1024, 1048576]).","category":"page"},{"location":"user_guide/parameters/#Path-Parameters","page":"Parameter Configuration","title":"Path Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nlibrary String Path to spectral library file\nms_data String Path to mass spectrometry data directory\nresults String Path to output results directory","category":"page"},{"location":"user_guide/parameters/#BuildSpecLib-Configuration","page":"Parameter Configuration","title":"BuildSpecLib Configuration","text":"","category":"section"},{"location":"user_guide/parameters/#FASTA-Input-and-Regex-Mapping","page":"Parameter Configuration","title":"FASTA Input and Regex Mapping","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Pioneer.jl now supports flexible FASTA input through GetBuildLibParams:","category":"page"},{"location":"user_guide/parameters/#Input-Options","page":"Parameter Configuration","title":"Input Options","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Single directory: Scans for all .fasta and .fasta.gz files\nSingle file: Directly uses the specified FASTA file\nMixed array: Any combination of directories and files","category":"page"},{"location":"user_guide/parameters/#Regex-Code-Mapping","page":"Parameter Configuration","title":"Regex Code Mapping","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"The regex patterns for parsing FASTA headers can be configured in three ways:","category":"page"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Single regex set for all files (default):\nGetBuildLibParams(out_dir, lib_name, [dir1, dir2, file1])\n# All FASTA files use the same default regex patterns\nCustom single regex set:\nGetBuildLibParams(out_dir, lib_name, [dir1, file1],\n    regex_codes = Dict(\n        \"accessions\" => \"^>(\\\\S+)\",\n        \"genes\" => \"GN=(\\\\S+)\",\n        \"proteins\" => \"\\\\s+(.+?)\\\\s+OS=\",\n        \"organisms\" => \"OS=(.+?)\\\\s+GN=\"\n    ))\n# All files use these custom patterns\nPositional mapping (one regex set per input):\nGetBuildLibParams(out_dir, lib_name, [uniprot_dir, custom_file],\n    regex_codes = [\n        Dict(\"accessions\" => \"^\\\\w+\\\\|(\\\\w+)\\\\|\", ...),  # For uniprot_dir files\n        Dict(\"accessions\" => \"^>(\\\\S+)\", ...)             # For custom_file\n    ])","category":"page"},{"location":"user_guide/parameters/#FASTA-Digest-Parameters","page":"Parameter Configuration","title":"FASTA Digest Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nmin_length Int Minimum peptide length (default: 7)\nmax_length Int Maximum peptide length (default: 30)\nmin_charge Int Minimum charge state (default: 2)\nmax_charge Int Maximum charge state (default: 4)\ncleavage_regex String Regular expression for cleavage sites (default: \"[KR][^_|$]\", to exclude cleavage after proline: \"[KR][^P\nmissed_cleavages Int Maximum allowed missed cleavages (default: 1)\nmax_var_mods Int Maximum variable modifications per peptide (default: 1)\nadd_decoys Boolean Generate decoy sequences (default: true)\nentrapment_r Float Ratio of entrapment sequences (default: 0)\nfasta_header_regex_accessions [String] Regex with a capture group for the accession, one per FASTA file\nfasta_header_regex_genes [String] Regex with a capture group for the gene name, one per FASTA file\nfasta_header_regex_proteins [String] Regex with a capture group for the protein name, one per FASTA file\nfasta_header_regex_organisms [String] Regex with a capture group for the organism, one per FASTA file","category":"page"},{"location":"user_guide/parameters/#NCE-Parameters","page":"Parameter Configuration","title":"NCE Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nnce Float Base normalized collision energy (default: 25.0)\ndefault_charge Int Default charge state for NCE calculations (default: 2)\ndynamic_nce Boolean Use charge-dependent NCE adjustments (default: true)","category":"page"},{"location":"user_guide/parameters/#Library-Parameters","page":"Parameter Configuration","title":"Library Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nrt_bin_tol Float Retention time binning tolerance in minutes (default: 1.0)\nfrag_bin_tol_ppm Float Fragment mass tolerance in PPM (default: 10.0)\nrank_to_score [Int] Intensity multipliers for ranked peaks (default: [8,4,4,2,2,1,1])\ny_start_index Int Starting index for y-ion annotation (default: 4)\nb_start_index Int Starting index for b-ion annotation (default: 3)\ny_start Int Minimum y-ion to consider (default: 3)\nb_start Int Minimum b-ion to consider (default: 2)\ninclude_p_index Boolean Include proline-containing index fragments (default: false)\ninclude_p Boolean Include proline-containing fragments (default: false)\nauto_detect_frag_bounds Boolean Auto-detect fragment mass bounds (default: true)\ncalibration_raw_file String Path to calibration raw file\nfrag_mz_min Float Minimum fragment m/z (default: 150.0)\nfrag_mz_max Float Maximum fragment m/z (default: 2020.0)\nprec_mz_min Float Minimum precursor m/z (default: 390.0)\nprec_mz_max Float Maximum precursor m/z (default: 1010.0)\nmax_frag_charge Int Maximum fragment ion charge (default: 3)\nmax_frag_rank Int Maximum fragment rank (default: 50)\nmin_frag_intensity Float Minimum relative fragment intensity (default: 0.00)\ninclude_isotope Boolean Include isotope peak annotations (default: false)\ninclude_internal Boolean Include internal fragment annotations (default: false)\ninclude_immonium Boolean Include immonium ion annotations (default: false)\ninclude_neutral_diff Boolean Include neutral loss annotations (default: true)\ninstrument_type String Instrument type for predictions (default: \"NONE\")\nprediction_model String Model for fragment predictions (default: \"altimeter\")","category":"page"},{"location":"user_guide/parameters/#Modification-Parameters","page":"Parameter Configuration","title":"Modification Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nvariable_mods.pattern [String] Amino acids to modify (default: [\"M\"])\nvariable_mods.mass [Float] Modification masses (default: [15.99491])\nvariable_mods.name [String] Modification identifiers (default: [\"Unimod:35\"])\nfixed_mods.pattern [String] Amino acids to modify (default: [\"C\"])\nfixed_mods.mass [Float] Modification masses (default: [57.021464])\nfixed_mods.name [String] Modification identifiers (default: [\"Unimod:4\"])","category":"page"},{"location":"user_guide/parameters/#Processing-Parameters","page":"Parameter Configuration","title":"Processing Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nmax_koina_requests Int Maximum concurrent Prosit API requests (default: 24)\nmax_koina_batch Int Maximum batch size for API requests (default: 1000)\nmatch_lib_build_batch Int Batch size for library building (default: 100000)","category":"page"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"note: Koina API Retry Behavior\nAs of version 0.1.13, Koina API retry warnings are now logged at debug level 2 instead of being shown to users by default. To see retry attempts during debugging, set debug_console_level: 2 in your SearchDIA parameters. The library build will only fail if all retry attempts are exhausted.","category":"page"},{"location":"user_guide/parameters/#Path-Parameters-2","page":"Parameter Configuration","title":"Path Parameters","text":"","category":"section"},{"location":"user_guide/parameters/","page":"Parameter Configuration","title":"Parameter Configuration","text":"Parameter Type Description\nfasta_paths [String] List of FASTA file paths\nfasta_names [String] Names for each FASTA file\ninclude_contaminants Boolean Append a contaminants FASTA to the build (default: true)\nout_dir String Output directory path\nlib_name String Base name for library files\nnew_lib_name String Name for updated library files\nout_name String Output filename\npredict_fragments Boolean Predict fragment intensities (default: true)","category":"page"},{"location":"user_guide/installation/#Installation-Guide","page":"Installation Guide","title":"Installation Guide","text":"","category":"section"},{"location":"user_guide/installation/#System-Requirements","page":"Installation Guide","title":"System Requirements","text":"","category":"section"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Julia: 1.10 or higher\nCPU: Multiple cores and threads recommended. Increasing the number of threads reduces computation time.\nRAM: >=16GB recommended. RAM availability should exceed the spectral library size by at least 4GB. For searching against the yeast proteome, as little as 6-8 GB may suffice.  \nStorage: SSD recommended. Available disk space at least double the total size of the .arrow formmated raw files to search. The .arrow files are usually ~1/2 the size of the vendor files. \nOperating System: Windows, Linux, or macOS","category":"page"},{"location":"user_guide/installation/#Installation","page":"Installation Guide","title":"Installation","text":"","category":"section"},{"location":"user_guide/installation/#End-User-Installation","page":"Installation Guide","title":"End-User Installation","text":"","category":"section"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Download the installer for your operating system from the releases page.\nRun the installer. It places a pioneer executable on your PATH.\nOn first launch:\nmacOS – Gatekeeper verifies the binary and the first run can take about a minute. Zipped binaries require manual Gatekeeper approval and are not recommended.\nVerify the installation:\npioneer --help","category":"page"},{"location":"user_guide/installation/#Docker","page":"Installation Guide","title":"Docker","text":"","category":"section"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Run Pioneer in a container without installing dependencies.","category":"page"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Pull the prebuilt image:\ndocker pull dennisgoldfarb/pioneer:latest\nExecute Pioneer inside the container, mounting a host directory (e.g. the current directory) to access data:\ndocker run --rm -it -v $(pwd):/work dennisgoldfarb/pioneer:latest pioneer --help\nReplace pioneer --help with any subcommand.\nTo build the image locally using the included Dockerfile:\ndocker build -t pioneer .","category":"page"},{"location":"user_guide/installation/#Development-Setup","page":"Installation Guide","title":"Development Setup","text":"","category":"section"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"To work on Pioneer itself, set up a local development environment.","category":"page"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Install Julia 1.10 or higher from julia.org.\nClone the repository:\ngit clone https://github.com/nwamsley1/Pioneer.jl.git\ncd Pioneer.jl\nStart Julia in the development environment and activate the project:\njulia --project=dev\npkg> develop ./\nIn the Julia REPL load Revise and Pioneer:\njulia> using Revise, Pioneer\nInstall PioneerConverter to convert Thermo RAW files to Arrow format.\nCall the main functions directly, e.g.\n# Option 1: Single FASTA directory (backward compatible)\nparams = GetBuildLibParams(out_dir, lib_name, fasta_dir)\nBuildSpecLib(params)\n\n# Option 2: Flexible input - files and/or directories\nparams = GetBuildLibParams(out_dir, lib_name, \n    [\"/path/to/dir1\", \"/path/to/file.fasta\", \"/path/to/dir2\"])\nBuildSpecLib(params)\nparams = GetSearchParams(\"library.poin\", \"ms_data\", \"results\")\nSearchDIA(params)","category":"page"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Subcommand Julia function\nparams-predict GetBuildLibParams\npredict BuildSpecLib\nparams-search GetSearchParams\nsearch SearchDIA\nconvert-raw PioneerConverter\nconvert-mzml convertMzML","category":"page"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"note: 'note'\nRevise enables hot reloading of code during development.","category":"page"},{"location":"user_guide/installation/#PioneerConverter","page":"Installation Guide","title":"PioneerConverter","text":"","category":"section"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Detailed installation and usage instructions for PioneerConverter are available in its documentation.","category":"page"},{"location":"user_guide/installation/#Next-Steps","page":"Installation Guide","title":"Next Steps","text":"","category":"section"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"After installation:","category":"page"},{"location":"user_guide/installation/","page":"Installation Guide","title":"Installation Guide","text":"Follow the Quick Start Tutorial.\nGenerate parameter files with pioneer params-predict or pioneer params-search, then edit them according to Parameter Configuration.","category":"page"},{"location":"development/macos_signing_streamline_plan/#macOS-Packaging-—-Signing/Notarization-Streamlining-Plan","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/#Goal","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Goal","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Cut macOS Build & Package time by only signing what’s necessary, avoiding deep re‑sign passes, and pruning non‑runtime files. Keep the output fully notarizable and identical in behavior at install/run time.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Current-State-(Summary)","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Current State (Summary)","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Workflow: .github/workflows/build_app_macos.yml runs on push/PR and on release via workflow_call.\nPackaging steps:\nBuild app layout under pkgroot/usr/local/Pioneer.\nCodesign all frameworks, then run a broad Mach‑O scan and sign every file that looks like a binary, then deep‑sign the entire tree again.\npkgbuild → productsign → notarytool --wait → stapler → upload artifacts.\nPain points:\nThe “sign everything” loop scans huge artifact trees (Qt, GR, Julia artifacts) and re‑signs many non‑runtime files (e.g., *.o) and duplicates signing work due to a final deep sign.\nSix‑hour job timeout indicates signing is a major bottleneck even before/alongside notarization.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Principles","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Principles","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Only sign code objects that ship and execute:\nApp bundle main binaries (*/Contents/MacOS/*)\nFramework bundles (*.framework)\nCLI executables under bin/ (executable bit)\nDynamic libraries/plugins (*.dylib, *.so, Qt/GR plugins)\nDo not sign build debris and non‑runtime files:\nObject files *.o, static archives *.a, intermediate objects-Release trees\nAvoid deep re‑sign at the end; keep targeted signing passes and a single verification step.\nReduce per‑file timestamp usage to lower network overhead; notarization of the final .pkg is the authoritative ticket.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Implementation-Steps","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Implementation Steps","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Restrict packaging triggers (optional but recommended)\nMake heavy packaging/signing/notarization release‑only:\nRemove push/pull_request triggers from build_app_macos.yml and keep workflow_call + workflow_dispatch.\nOr keep push/PR but gate the four heavy steps (codesign, notarize, staple, upload signed pkg) with if: startsWith(github.ref, 'refs/tags/v').\nPre‑prune non‑runtime files\nBefore signing, delete object/static files and intermediate build dirs within the payload:\n# Remove object files and archives\nfind \"$PKGROOT/usr/local/$APP\" -type f \\( -name '*.o' -o -name '*.a' \\) -delete\n# Remove known intermediate dirs\nfind \"$PKGROOT/usr/local/$APP\" -type d -name 'objects-Release' -prune -exec rm -rf {} +\nTargeted signing passes (replace broad scan + deep sign)\nFrameworks (first):\nfind \"$PKGROOT/usr/local/$APP\" -type d -name '*.framework' -print0 | \\\n  while IFS= read -r -d '' fw; do\n    echo \"Signing framework: $fw\"\n    codesign --force --options runtime \\\n      --entitlements src/build/osx/entitlements.plist \\\n      --sign \"$CODESIGN_IDENTITY\" \"$fw\"\n  done\nApp bundle binaries (GR apps):\nfind \"$PKGROOT/usr/local/$APP\" -type f -path '*/Contents/MacOS/*' -print0 | \\\n  while IFS= read -r -d '' f; do\n    echo \"Signing app binary: $f\"\n    codesign --force --options runtime \\\n      --entitlements src/build/osx/entitlements.plist \\\n      --sign \"$CODESIGN_IDENTITY\" \"$f\"\n  done\nCLI executables in bin/:\nif [ -d \"$PKGROOT/usr/local/$APP/bin\" ]; then\n  find \"$PKGROOT/usr/local/$APP/bin\" -type f -perm -111 -print0 | \\\n    while IFS= read -r -d '' f; do\n      echo \"Signing CLI: $f\"\n      codesign --force --options runtime \\\n        --entitlements src/build/osx/entitlements.plist \\\n        --sign \"$CODESIGN_IDENTITY\" \"$f\"\n    done\nfi\nDynamic libraries and plugins (avoid frameworks):\nfind \"$PKGROOT/usr/local/$APP\" \\( -name '*.dylib' -o -name '*.so' \\) \\\n  -not -path '*/.framework/*' -print0 | \\\n  while IFS= read -r -d '' f; do\n    echo \"Signing dylib/plugin: $f\"\n    codesign --force --options runtime \\\n      --entitlements src/build/osx/entitlements.plist \\\n      --sign \"$CODESIGN_IDENTITY\" \"$f\"\n  done\nNotes:\nDeliberately omit --timestamp on per‑file signing to reduce latency (final notarized pkg is authoritative).\nNo final codesign --deep over the whole tree.\nVerify signatures once\ncodesign --verify --deep --strict --verbose=2 \"$PKGROOT/usr/local/$APP\"\nBuild, sign, and notarize package (unchanged)\npkgbuild → productsign → notarytool --wait → stapler.\nConsider adding a readable timeout to the Notarize step (e.g., 120 minutes) for earlier feedback, while recognizing hosted runner hard limit is 6h.\nLogging improvements\nPrint counts per signing pass (e.g., frameworks signed, app binaries signed, dylibs signed) to make hotspots visible in run logs.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Optional-Cleanup-(further-reductions)","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Optional Cleanup (further reductions)","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Clear extended attributes (if present) before signing to avoid quarantine flag propagation:\nxattr -rd com.apple.quarantine \"$PKGROOT/usr/local/$APP\" || true\nRemove unused Qt build artifacts (if safe): e.g., plugins/*/objects-* directories — already covered by the generic prune above.\nIf verification fails due to a missed plugin path in a specific release, add a targeted mini‑pass for that subpath rather than re‑introducing --deep globally.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Risk-Assessment-and-Mitigations","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Risk Assessment and Mitigations","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Risk: A required code object is missed by the targeted passes.\nMitigation: Keep a strict codesign --verify --deep --strict check. In failure cases, inspect the path and add a specific signing rule for that subtree.\nRisk: Removing --timestamp on individual files could reduce trust chain detail.\nMitigation: Final notarization of the .pkg is the requirement for Gatekeeper; per‑file timestamp is not required.\nRisk: Accidental deletion of needed files during prune.\nMitigation: Restrict prune to *.o, *.a, and objects-Release only; do not remove .dylib/.so or app resources.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Validation-Plan","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Validation Plan","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Dry run on workflow_dispatch with the new signing passes; capture per‑step durations.\nConfirm codesign --verify --deep --strict passes.\nConfirm notarization status Accepted and stapling succeeds.\nSmoke test: install .pkg on a clean macOS VM for both x64 and arm64; run CLI and launch GR apps.\nCompare job total time vs. baseline; target >50% reduction.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Rollout-and-Backout","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Rollout and Backout","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Rollout: Merge workflow change, test on a release tag candidate.\nBackout: Revert to previous workflow or temporarily add a final --deep pass for a subset path if a corner case appears. Keep the old logic in VCS for quick rollback.","category":"page"},{"location":"development/macos_signing_streamline_plan/#Follow‑ups-(Optional)","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"Follow‑ups (Optional)","text":"","category":"section"},{"location":"development/macos_signing_streamline_plan/","page":"macOS Packaging — Signing/Notarization Streamlining Plan","title":"macOS Packaging — Signing/Notarization Streamlining Plan","text":"Make packaging fully release‑only to keep CI lightweight on pushes/PRs.\nAdd summaries to job logs with itemized counts and durations per signing phase.","category":"page"},{"location":"advanced/performance/#Performance-Tuning","page":"Performance Tuning","title":"Performance Tuning","text":"","category":"section"},{"location":"development/log_message_truncation_plan/#Log-Message-Truncation-—-Implementation-Plan","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"","category":"section"},{"location":"development/log_message_truncation_plan/#Goal","page":"Log Message Truncation — Implementation Plan","title":"Goal","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Prevent any single log write from ballooning the log files (e.g., when an error message accidentally embeds a giant array). Add a configurable per‑message byte cap so long messages are truncated safely before being written to console or files.","category":"page"},{"location":"development/log_message_truncation_plan/#Scope-and-Non‑Goals","page":"Log Message Truncation — Implementation Plan","title":"Scope and Non‑Goals","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"In scope: Truncate individual log messages to a maximum size (bytes) across all logging sinks (console, essential, debug, warnings).\nNot in scope (but considered later): Log rotation, global file size caps, duplicate‑message throttling.","category":"page"},{"location":"development/log_message_truncation_plan/#Configuration","page":"Log Message Truncation — Implementation Plan","title":"Configuration","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Add a new parameter: logging.max_message_bytes.\nType: integer (bytes)\nDefault: 4096 (4 KB)\nMin allowed: 1024 (1 KB) — guards against unrealistic settings.\nMax allowed: e.g. 1048576 (1 MB) — sanity cap.\nEnvironment override: if ENV[\"PIONEER_MAX_LOG_MSG_BYTES\"] is set, it takes precedence (validated and clamped to [min, max]).","category":"page"},{"location":"development/log_message_truncation_plan/#Where-to-put-it","page":"Log Message Truncation — Implementation Plan","title":"Where to put it","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Update the default JSON configuration (example configs):\nassets/example_config/defaultSearchParams.json (or the currently recommended default JSON file) under the existing logging block:\n\"logging\": {\n  \"debug_console_level\": 0,\n  \"max_message_bytes\": 4096\n}\nUpdate the user docs at docs/src/user_guide/parameters.md to document the new field (default, meaning, and ENV override).","category":"page"},{"location":"development/log_message_truncation_plan/#Design","page":"Log Message Truncation — Implementation Plan","title":"Design","text":"","category":"section"},{"location":"development/log_message_truncation_plan/#Helper-function:-truncate*for*log","page":"Log Message Truncation — Implementation Plan","title":"Helper function: truncateforlog","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Add a small helper that ensures UTF‑8 safe truncation by bytes, with a clear suffix annotation:","category":"page"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"const MAX_LOG_MSG_BYTES = Ref{Int}(4096)  # set at startup from params/env\n\nfunction truncate_for_log(msg::String; max_bytes::Int=MAX_LOG_MSG_BYTES[])\n    # Fast path\n    nb = ncodeunits(msg)\n    if nb <= max_bytes\n        return msg\n    end\n\n    # Walk codepoints and accumulate bytes until adding the next char would exceed max_bytes\n    n = 0\n    lastidx = 0\n    @inbounds for (i, ch) in enumerate(eachindex(msg))\n        cu = ncodeunits(msg[i])  # bytes for this char\n        if n + cu > max_bytes\n            break\n        end\n        n += cu\n        lastidx = i\n    end\n\n    # Fallback if something odd happens\n    lastidx == 0 && return msg[1:prevind(msg, end)] * \" … [truncated]\"\n\n    prefix = @views String(msg[1:lastidx])\n    omitted = nb - n\n    return string(prefix, \" … [truncated \", omitted, \" bytes]\")\nend","category":"page"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Notes:","category":"page"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Uses byte accounting (ncodeunits) for accuracy.\nEnsures we never split a multi‑byte codepoint by advancing at character boundaries (eachindex).\nAdds an explicit suffix with the number of omitted bytes.","category":"page"},{"location":"development/log_message_truncation_plan/#Integration-points","page":"Log Message Truncation — Implementation Plan","title":"Integration points","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Centralize all message writes through truncate_for_log:\nuser_info, user_warn, user_error, user_print in src/Pioneer.jl.\nAny place that builds a message via sprint(showerror, e, bt) must pass the result through truncate_for_log before printing.","category":"page"},{"location":"development/log_message_truncation_plan/#Example-wiring-(conceptual,-not-exact-code):","page":"Log Message Truncation — Implementation Plan","title":"Example wiring (conceptual, not exact code):","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"function user_error(msg::String, file::String=\"\", line::String=\"\", mod::String=\"\")\n    msg_trunc = truncate_for_log(msg)\n    # console\n    println(msg_trunc)\n    # files (ESSENTIAL/CONSOLE/DEBUG/WARNINGS): also use msg_trunc\nend\n\n# For exception printing\nerr_str = sprint(showerror, e, bt)\nprintln(truncate_for_log(err_str))","category":"page"},{"location":"development/log_message_truncation_plan/#Parameter-loading","page":"Log Message Truncation — Implementation Plan","title":"Parameter loading","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Extend the parameter parsing to read params.logging.max_message_bytes into the runtime config.\nOn SearchDIA start, set MAX_LOG_MSG_BYTES[] to:\nENV override if present and valid; else\nparams value; else default 4096.\nClamp to [1024, 1048576].","category":"page"},{"location":"development/log_message_truncation_plan/#Backward-Compatibility","page":"Log Message Truncation — Implementation Plan","title":"Backward Compatibility","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Default behavior (4096 bytes) ensures most messages are unaffected. Only unusually long messages will be truncated.\nExisting configs without the new field continue to work (default applied).\nENV override offers a runtime escape hatch for ops (e.g., set lower during CI).","category":"page"},{"location":"development/log_message_truncation_plan/#Testing-Plan","page":"Log Message Truncation — Implementation Plan","title":"Testing Plan","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Unit‑style tests (lightweight):\nTruncate a short ASCII message → unchanged.\nTruncate a long ASCII message → length limited; suffix present.\nTruncate a long UTF‑8 string with multibyte characters → no invalid UTF‑8; suffix present.\nTruncate a gigantic error string created via sprint(showerror, …) → limited, suffix present.\nIntegration sanity:\nSimulate a failure that previously dumped huge arrays (e.g., BoundsError with big vector) and verify the debug/warnings logs contain a short, truncated message instead of megabytes of data.","category":"page"},{"location":"development/log_message_truncation_plan/#Rollout-Steps","page":"Log Message Truncation — Implementation Plan","title":"Rollout Steps","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Add logging.max_message_bytes to example/default JSON(s).\nUpdate parameter parsing (where logging settings are read) to store the value.\nAdd MAX_LOG_MSG_BYTES ref and truncate_for_log helper in src/Pioneer.jl.\nApply truncation in user_info, user_warn, user_error, user_print, and any direct sprint(showerror, …) prints.\nClamp and initialize from params/env at SearchDIA startup.\nUpdate documentation (docs/src/user_guide/parameters.md).\n(Optional) Add a minimal test that asserts truncation occurs.","category":"page"},{"location":"development/log_message_truncation_plan/#Risks-and-Mitigations","page":"Log Message Truncation — Implementation Plan","title":"Risks and Mitigations","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Risk: Truncation could hide critical context.\nMitigation: Preserve head of message; append explicit “truncated N bytes”. Operators can raise the cap via config/env if needed.\nRisk: Performance overhead of character iteration.\nMitigation: Only executed when message exceeds threshold; normal messages take the fast path.","category":"page"},{"location":"development/log_message_truncation_plan/#Future-Extensions-(Optional)","page":"Log Message Truncation — Implementation Plan","title":"Future Extensions (Optional)","text":"","category":"section"},{"location":"development/log_message_truncation_plan/","page":"Log Message Truncation — Implementation Plan","title":"Log Message Truncation — Implementation Plan","text":"Duplicate warning throttling: per‑window max repeats to prevent millions of identical lines.\nGlobal log size guard: after each file/phase, abort if logs exceed a threshold (e.g., 200 MB).\nSimple rotation: when a log exceeds limit, rename to *.YYYYMMDD-HHMMSS.log and start fresh (keep at most N).","category":"page"},{"location":"user_guide/quickstart/#Quick-Start-Tutorial","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"","category":"section"},{"location":"user_guide/quickstart/#Basic-Workflow","page":"Quick Start Tutorial","title":"Basic Workflow","text":"","category":"section"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"Pioneer performs three major steps:","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"Convert vendor MS files into the Arrow format using PioneerConverter.\nBuild in silico spectral libraries using FASTA files and the Koina server.\nSearch DIA experiments using a spectral library and the MS data files.","category":"page"},{"location":"user_guide/quickstart/#Pioneer-Converter","page":"Quick Start Tutorial","title":"Pioneer Converter","text":"","category":"section"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"Pioneer operates on MS/MS data stored in the Apache Arrow IPC format. Use the bundled PioneerConverter via the CLI to convert Thermo RAW files:","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"pioneer convert-raw /path/to/raw/or/folder","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"This subcommand accepts either a single .raw file or a directory of files. See the PioneerConverter repository for additional options such as thread count and output paths.","category":"page"},{"location":"user_guide/quickstart/#MzML-to-Arrow-IPC-(Sciex)","page":"Quick Start Tutorial","title":"MzML to Arrow IPC (Sciex)","text":"","category":"section"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"For mzML-formatted data, use:","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"pioneer convert-mzml /path/to/mzml/or/folder","category":"page"},{"location":"user_guide/quickstart/#Starting-Pioneer","page":"Quick Start Tutorial","title":"Starting Pioneer","text":"","category":"section"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"After installation, Pioneer is accessed from the command line. Running pioneer --help displays available subcommands:","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"pioneer [options] <subcommand> [subcommand-args...]","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"Subcommands include search, predict, params-search, params-predict, convert-raw, and convert-mzml. On the first launch macOS performs a one-time Gatekeeper check.","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"A minimal end-to-end workflow is:","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"# Traditional approach with single FASTA directory\npioneer params-predict lib_dir lib_name fasta_dir --params-path=predict_params.json\n\n# Or with new flexible FASTA input (mixing directories and files)\n# Note: CLI support for mixed inputs requires editing the JSON parameter file\npioneer params-predict lib_dir lib_name fasta_dir --params-path=predict_params.json\n# Then edit predict_params.json to set fasta_paths to include specific files\n\npioneer predict predict_params.json\npioneer convert-raw raw_dir\npioneer params-search library.poin ms_data_dir results_dir --params-path=search_params.json\npioneer search search_params.json","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"This sequence builds a predicted spectral library, converts vendor files to Arrow, generates search parameters, and searches the experiment.","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"tip: Advanced FASTA Input\nWhen using the Julia API directly, you can specify mixed FASTA sources:params = GetBuildLibParams(out_dir, lib_name, \n    [\"/path/to/uniprot/\", \"/custom/proteins.fasta\"])","category":"page"},{"location":"user_guide/quickstart/","page":"Quick Start Tutorial","title":"Quick Start Tutorial","text":"params-predict and params-search create template JSON files. Edit these configurations to suit your experiment before running predict or search. See Parameter Configuration for a description of each option.","category":"page"},{"location":"advanced/algorithms/#algorithm-documentation","page":"Algorithm Documentation","title":"Algorithm Documentation","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/#Plan:-Robust-MS1MS2-Join-in-SecondPassSearch","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/#Context","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Context","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"Location: process_search_results! in SecondPassSearch.\nCurrent behavior: After filter!(x -> x.best_scan, psms), we keep MS2 PSM rows intended to represent the apex per precursor_idx. There can still be multiple MS2 rows per precursor_idx if different :isotopes_captured values are retained. We want to attach a single MS1 PSM row per precursor — the one whose RT is closest to the MS2 apex RT — and replicate that same MS1 info to all MS2 rows of that precursor (including across distinct :isotopes_captured).\nCurrent implementation sketch: Select closest MS1 row by scanning psms inside a combine(groupby(ms1_psms, :precursor_idx)) body; then perform a left join on :precursor_idx only.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Goals","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Goals","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"Deterministic selection of the “best” MS1 PSM per key (precursor, and run), defined as minimal |RTms1 − RTms2_apex|.\nCorrect replication: When multiple MS2 rows exist for the same precursor (e.g., different :isotopes_captured), the same chosen MS1 row is attached to each MS2 row.\nRobust handling across runs: Do not cross-contaminate matches between different files/runs. Use :ms_file_idx alongside :precursor_idx where applicable.\nPerformance: Avoid O(N^2) scans of psms inside group loops; precompute and vectorize where possible.\nData hygiene: Optionally handle non-finite MS1 metrics (Inf/NaN) before downstream scoring/ML.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Proposed-Design","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Proposed Design","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"Define join key\nUse join_keys = [:ms_file_idx, :precursor_idx] if available; otherwise fall back to [:precursor_idx] in strictly single-run contexts.\nRationale: Ensures MS1/ MS2 are matched within the same run.\nPrecompute canonical MS2 apex RT per key\nAfter best_scan filtering, derive a single MS2 reference RT per join_keys:\nIf psms has one row per key: ms2_apex_rt = psms.rt directly.\nIf multiple rows per key remain (e.g., differing :isotopes_captured): choose a deterministic reference, e.g., the row with max :prob (or simply the first after sorting by :prob desc, then :rt).\nImplementation idea:\nms2_apex = combine(groupby(psms, join_keys), [:prob, :rt] => ((p, r) -> r[argmax(p)]) => :ms2_apex_rt)\nIf :prob not suitable, choose first(r) after sorting group_df by :prob desc.\nJoin MS2 apex RT into MS1 candidates\nEnsure ms1_psms has :rt; if missing, compute via getRetentionTime(spectra, scan_idx) (already present in the code).\nLeft-join: ms1_psms_with_rt = leftjoin(ms1_psms, ms2_apex, on=join_keys) to add :ms2_apex_rt.\nCompute absolute RT difference: :rt_diff = abs.(ms1_psms_with_rt.rt .- ms1_psms_with_rt.ms2_apex_rt); be careful with missing :ms2_apex_rt (skip such groups or keep as missing for later prune).\nSelect one MS1 row per key (closest RT)\nGroup ms1_psms_with_rt by join_keys and choose the row with the minimum :rt_diff per group. Deterministic tie-breaker: if multiple rows share the same minimum :rt_diff, pick the one with higher :prob (if present) or the first by :scan_idx.\nImplementation idea:\nPrecompute per-group boolean mask of is_min using argmin on :rt_diff.\nOr use combine(..., :rt_diff => argmin => :idx) then groupby-indexed lookup.\nResult: ms1_psms_best with a single row per join_keys.\nFinal join to replicate MS1 across MS2 rows\nLeft-join original psms with ms1_psms_best on join_keys only (do NOT include :isotopes_captured so the same MS1 row is replicated across all MS2 isotopic rows of the precursor):\npsms = leftjoin(psms, ms1_psms_best, on=join_keys, makeunique=true, renamecols = \"\" => \"_ms1\")\nThis satisfies the requirement: “give the MS1 data to both rows” when multiple MS2 rows exist for a precursor.\nOptional: sanitize non-finite MS1 columns\nReplace Inf/-Inf/NaN within the appended _ms1 columns to missing (or clamp to numeric bounds) before ML/scoring stages that assume finiteness.\nImplementation idea: find columns ending with _ms1 and apply replace!(col, x -> isfinite(x) ? x : missing) for Float columns.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Complexity-and-Performance","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Complexity and Performance","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"The plan avoids nested lookups of psms from inside a group loop. Instead, it computes a compact ms2_apex table once, joins it into MS1 candidates, and uses groupwise argmin. This scales linearly with the number of MS1 and MS2 records and leverages DataFrames’ optimized grouping/joins.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Edge-Cases","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Edge Cases","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"No MS1 candidates for a key: The left join produces missing MS1 columns; acceptable.\nNo MS2 apex record for a key: ms2_apex_rt will be missing; either drop such groups from ms1_psms_best or allow the final join to propagate missing MS1 values.\nMultiple runs: Ensure join_keys includes :ms_file_idx (or whichever run identifier is canonical) to avoid cross-run matches.\nMultiple MS2 rows per key: Intentional — the same MS1 row is replicated to each MS2 row after the final join.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Testing-Plan","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Testing Plan","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"Unit tests (DataFrames-level)\nSingle-run, single-precursor: multiple MS1 candidates; verify nearest-by-RT selection.\nSingle-run, multiple MS2 rows (different :isotopes_captured): confirm the same MS1 row attaches to all.\nMulti-run: include two runs with overlapping :precursor_idx; verify join respects :ms_file_idx.\nTies on RT difference: ensure deterministic tie-breaker (e.g., highest :prob or smallest :scan_idx).\nMissing :rt in MS1: verify it is populated via getRetentionTime path.\nNon-finite values: introduce Inf in an MS1 metric and verify sanitization logic (if enabled) results in missing.\nIntegration tests\nCraft small synthetic MS2/ MS1 sets flowing through the relevant part of SecondPassSearch and assert the joined columns and row counts.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Pseudocode-(for-orientation-only)","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Pseudocode (for orientation only)","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"# After best_scan filtering in psms\njoin_keys = hasproperty(psms, :ms_file_idx) ? [:ms_file_idx, :precursor_idx] : [:precursor_idx]\n\n# 1) Compute ms2 apex RT per key (deterministic)\nms2_apex = combine(groupby(psms, join_keys)) do g\n    g_sorted = sort(g, [:prob, :rt], rev=[true, false])\n    (; ms2_apex_rt = g_sorted.rt[1])\nend\n\n# 2) Ensure ms1_psms has :rt\nif !in(:rt, names(ms1_psms))\n    ms1_psms.rt = [getRetentionTime(spectra, i) for i in ms1_psms.scan_idx]\nend\n\n# 3) Join ms2 apex rt into ms1 candidates and compute |Δrt|\nms1p = leftjoin(ms1_psms, ms2_apex, on=join_keys)\nms1p.rt_diff = abs.(ms1p.rt .- ms1p.ms2_apex_rt)\n\n# 4) Pick closest per key\nms1p_sorted = sort(ms1p, [:rt_diff, :prob, :scan_idx], rev=[false, true, false])\nms1_best = combine(groupby(ms1p_sorted, join_keys)) do g\n    g[1:1, :]\nend\n\n# 5) Final join: replicate MS1 across MS2 rows per key\npsms = leftjoin(psms, ms1_best, on=join_keys, makeunique=true, renamecols => (\"\" => \"_ms1\"))\n\n# 6) Optional: sanitize appended _ms1 float columns","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Rollout","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Rollout","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"Implement as a focused refactor inside process_search_results!:\nIntroduce a small helper (e.g., select_closest_ms1_per_precursor(ms1_psms, psms, spectra)) for clarity and unit-testability.\nKeep the join keys configurable based on the presence of run identifiers.\nAdd tests under test/Routines/SearchDIA/... mirroring this path.","category":"page"},{"location":"development/secondpass_ms1_join_plan/#Notes","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Notes","text":"","category":"section"},{"location":"development/secondpass_ms1_join_plan/","page":"Plan: Robust MS1→MS2 Join in SecondPassSearch","title":"Plan: Robust MS1→MS2 Join in SecondPassSearch","text":"This plan does not change the conceptual behavior (still “closest RT” MS1 attached), but makes it deterministic, scalable, and aligned with multi-run data. It also explicitly supports the case where multiple MS2 rows per precursor should receive the same MS1 data.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"In‑Memory MBR Candidate Labeling — Bug Analysis and Fix Proposal","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"This note explains why the in‑memory path is labeling far too many rows as match‑between‑runs (MBR) transfer candidates and how to correct it. The focus is on the in‑memory scorer in percolatorSortOf.jl and how it differs from the out‑of‑memory path that uses the correct logic.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Where Candidates Are Labeled (In‑Memory)","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"File: src/utils/ML/percolatorSortOf.jl\nFunction: sort_of_percolator_in_memory!\nLocation: near the end of the function, after cross‑validation models produce final probabilities and just before the MBR features and final write‑back.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Current code (abridged, as present in this branch):","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"# Determine which precursors failed the q-value cutoff prior to MBR\nqvals_prev = Vector{Float32}(undef, length(nonMBR_estimates))\nget_qvalues!(nonMBR_estimates, psms.target, qvals_prev)\npass_mask = (nonMBR_estimates .<= max_q_value_lightgbm_rescore)\nprob_thresh = any(pass_mask) ? minimum(nonMBR_estimates[pass_mask]) : typemax(Float32)\n\n# Label as transfer candidates only those failing the q-value cutoff but\n# whose best matched pair surpassed the passing probability threshold.\npsms[!, :MBR_transfer_candidate] .= .!pass_mask .&\n                                    (psms.MBR_max_pair_prob .>= prob_thresh)\n\n# Use the final MBR probabilities for all precursors\npsms[!, :prob] = MBR_estimates","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Note: nonMBR_estimates are probabilities (higher=better). max_q_value_lightgbm_rescore is a q‑value threshold (≈0.01) — a different scale.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"What’s Wrong (Scale Mismatch)","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"pass_mask = (nonMBR_estimates .<= max_q_value_lightgbm_rescore) compares probabilities to a q‑value threshold. Since true positives have large probabilities (e.g., 0.8, 0.9), the condition p ≤ 0.01 is almost never true.\nConsequence: pass_mask is mostly false; .!pass_mask becomes “almost everyone”.\nprob_thresh = minimum(nonMBR_estimates[pass_mask]) is taken over a tiny set (often empty). If any are present, they tend to be extremely small (≈0), so the condition MBR_max_pair_prob ≥ prob_thresh is trivially satisfied by most rows.\nNet effect: MBR_transfer_candidate is set to true for the vast majority of rows, which matches your observation:","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"FTR probability threshold: 0.8703515\nNum passing candidate transfers: 1811822 out of 1949436","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"This happens because the candidate set (where MBR_transfer_candidate=true) includes almost every row, forcing the FTR threshold τ high to keep the empirical ratio below α.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Correct Intent (Reference Implementation)","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"The out‑of‑memory path uses the correct logic inside update_mbr_probs! (same file):","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"function update_mbr_probs!(df::AbstractDataFrame, probs::AbstractVector{Float32}, qval_thresh::Float32)\n    prev_qvals = similar(df.prob)\n    get_qvalues!(df.prob, df.target, prev_qvals)   # compute q-values\n    pass_mask = (prev_qvals .<= qval_thresh) .& df.target\n    prob_thresh = any(pass_mask) ? minimum(df.prob[pass_mask]) : typemax(Float32)\n    df[!, :MBR_transfer_candidate] = (prev_qvals .> qval_thresh) .&   # use q-values\n                                     (df.MBR_max_pair_prob .>= prob_thresh)\n    df[!, :prob] = probs\n    return df\nend","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Two key differences from the in‑memory code above:\npass_mask is computed with q‑values prev_qvals, not with probabilities.\nprob_thresh is derived from the probabilities of the passing set (as intended), but the passing set is defined by qval_thresh, not a probability threshold.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Impact of the Bug","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Candidate set is massively inflated in the in‑memory path, causing:\nVery high τ even at α = 0.01.\nLarge number of “candidate transfers” reported, which is misleading and slows down filtering.\nDownstream clamping affects almost all rows, reducing the discriminative power of MBR.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Minimal Fix (In‑Memory Path)","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Replace the probability‑based pass_mask with a q‑value‑based mask (as in update_mbr_probs!). Proposed snippet drop‑in for sort_of_percolator_in_memory!:","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"# 1) Compute q-values of non-MBR predictions\nqvals_prev = Vector{Float32}(undef, length(nonMBR_estimates))\nget_qvalues!(nonMBR_estimates, psms.target, qvals_prev)\n\n# 2) Build pass/fail by q-value threshold\npass_mask = (qvals_prev .<= max_q_value_lightgbm_rescore) .& psms.target\nprob_thresh = any(pass_mask) ? minimum(nonMBR_estimates[pass_mask]) : typemax(Float32)\n\n# 3) Label transfer candidates: failed q-value but paired to a strong donor\npsms[!, :MBR_transfer_candidate] .= (qvals_prev .> max_q_value_lightgbm_rescore) .&\n                                    (psms.MBR_max_pair_prob .>= prob_thresh)","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"This aligns the in‑memory behavior with the out‑of‑memory update_mbr_probs! and ensures the candidate set is limited to plausible transfers.","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Verification Steps","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Add a short diagnostic around labeling (in‑memory):\nCount of pass_mask, candidate_count = sum(MBR_transfer_candidate), and prob_thresh.\nSanity: candidatecount should be a minority of all rows; `probthresh` should be a realistic boundary (e.g., near the minimum probability among passing targets).","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"Summary","category":"page"},{"location":"advanced/mbr_in_memory_candidate_labeling/","page":"-","title":"-","text":"The in‑memory path mistakenly compares probabilities to a q‑value threshold, inflating MBR_transfer_candidate.\nThe out‑of‑memory path uses the correct q‑value based mask.\nSwitching the in‑memory logic to use q‑values (as shown above) restores consistency and reduces the candidate set, leading to reasonable τ at α=0.01.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pioneer and its companion tool Altimeter are an open-source and performant solution for analysis of protein MS data acquired by data-independent acquisition (DIA). Poineer includes routines for searching DIA experments from Thermo and Sciex instruments and for building spectral libraries using the Koina interface. Given a spectral library of precursor fragment ion intensities and retention time estimates, Pioneer identifies and quantifies peptides from the library in the data. ","category":"page"},{"location":"#Design-Goals","page":"Home","title":"Design Goals","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Open-Source: Pioneer is completely open source. \nCross-Platform: Pioneer and the vendor-specific file conversion tool run on Linux, MacOS, and Windows\nHigh-Performance: Pioneer achieves high sensitivity, FDR control, quantitative precision and accuracy on benhcmark datat-sets \nScalability: Memory consumption and speed should remain constant as the number of raw files in an anslysis grows. Pioneer should scale to very large experiments with hundreds to thousands of raw files (experimental)\nFast: Pioneer searches data several times faster than it can be aquired and faster than state-of-the-art search tools.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pioneer and Altimeter build on previous search engines and introduce several new concepts:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Spectral Library Prediction with Koina: Using Koina Pioneer can construct fully predicted spectral libraries given an internet connection and a FASTA file with protein sequences. Pioneer uses Chronologer to predict peptide retention times and is optimized to use Altimeter for fragment ion intensity predictions.\nCollision Energy Independent Spectral Libraries: Rather than predicting a single intensity value for each fragment ion, Altimeter predicts 4 B-spline coefficients. Evaluating the fragment splines at a given collision energy gives a fragment ion intensity. Pioneer calibrates the library to find the optimal collision energy value to use for each MS data file in an experiment. In this way, it is possible to use a single spectral library for different instruments and scan settings. \nFragment Isotope Correction: Fragment isotope distributions depend on precursor isotope distributions as distorted by quadrupole mass filtering. Altimeter addresses this by predicting total fragment ion intensities rather than monoisotopic ones. Pioneer then accurately re-isotopes these library spectra using methods from Goldfarb et al.. This is particularly important for narrow-window DIA methods where precursor isotopic envelopes frequently straddle multiple windows.\nQaudrupole Transmission Modeling: For narrow-window data, Pioneer can optionally estimate a quadrupole-transmission efficiency function for more accurate re-isotoping. \nIntensity-Aware Fragment Index Search: Pioneer implements a fast fragment index search inspired by MSFragger and Sage. Pioneer's implementation uniquely leverages accurate fragment intensity predictions from in silico libraries to improve both speed and specificity of the search.\nLinear Regression onto Library Templates: Pioneer explains each observed mass spectrum as a linear combination of template spectra from the library. To reduce quantitative bias from interfering signals, Pioneer minimizes the pseudo-Huber loss rather than squared error. This provides robust quantification even in complex spectra. For other examples of linear regression applied to DIA analyses, see Specter and Chimerys.\nScalability: Pioneer was designed to scale to large experiments with many MS data files. Memory consumption remains constant as the number of data files in an experiment grows large. ","category":"page"},{"location":"#Quick-Links","page":"Home","title":"Quick Links","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Installation Guide\nQuick Start Tutorial","category":"page"},{"location":"#Authors-and-Development","page":"Home","title":"Authors and Development","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pioneer is developed and maintained by:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nathan Wamsley (Major Lab/Goldfarb Lab, Washington University)\nDennis Goldfarb (Goldfarb Lab, Washington University)","category":"page"},{"location":"#Contact","page":"Home","title":"Contact","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For questions about Pioneer or to collaborate, please contact:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Nathan Wamsley (wamsleynathan@gmail.com)\nDennis Goldfarb (dennis.goldfarb@wustl.edu)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For toubleshooting use the Issues page on github. To critique methods or propose features use the Discussions page.","category":"page"},{"location":"#Exported-Methods","page":"Home","title":"Exported Methods","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Pioneer.SearchDIA","page":"Home","title":"Pioneer.SearchDIA","text":"SearchDIA(params_path::String)\n\nMain entry point for the DIA (Data-Independent Acquisition) search workflow. Executes a series of SearchMethods and generates performance metrics.\n\nParameters:\n\nparams_path: Path to JSON configuration file containing search parameters\n\nOutput:\n\nGenerates a log file in the results directory\nLong and wide-formatted tables (.arrow and .csv) for protein-group and precursor level id's and quantitation.\nReports timing and memory usage statistics\n\nExample:\n\njulia> SearchDIA(\"/path/to/config.json\")\n==========================================================================================\nSarting SearchDIA\n==========================================================================================\n\nStarting search at: 2024-12-30T14:01:01.510\nOutput directory: ./../data/ecoli_test/ecoli_test_results\n[ Info: Loading Parameters...\n[ Info: Loading Spectral Library...\n .\n .\n .\n\nIf it does not already exist, SearchDIA creates the user-specified results_dir and generates quality control plots, data tables, and logs.\n\nresults_dir/\n├── pioneer_search_log.txt\n├── qc_plots/\n│   ├── collision_energy_alignment/\n│   │   └── nce_alignment_plots.pdf\n│   ├── quad_transmission_model/\n│   │   ├── quad_data\n│   │   │   └── quad_data_plots.pdf\n│   │   └── quad_models\n│   │       └── quad_model_plots.pdf\n│   ├── rt_alignment_plots/\n│   │   └── rt_alignment_plots.pdf\n│   ├── mass_error_plots/\n│   │   └── mass_error_plots.pdf\n│   └── QC_PLOTS.pdf\n├── precursors_long.arrow\n├── precursors_long.tsv\n├── precursors_wide.arrow\n├── precurosrs_wide.tsv\n├── protein_groups_long.arrow\n├── protein_groups_long.tsv\n├── protein_groups_wide.arrow\n└── protein_groups_wide.tsv\n\n\n\n\n\n","category":"function"},{"location":"#Pioneer.GetSearchParams","page":"Home","title":"Pioneer.GetSearchParams","text":"GetSearchParams(lib_path::String, ms_data_path::String, results_path::String; \n               params_path::Union{String, Missing} = missing,\n               simplified::Bool = true)\n\nCreates a search parameter configuration file with user-specified paths.\n\nThe function loads default parameters from either the simplified or full JSON template (from assets/example_config/) and creates a customized parameter file with the user's file paths. All other parameters retain their default values and can be modified later.\n\nArguments:\n\nlib_path: Path to the spectral library file (.poin)\nmsdatapath: Path to the MS data directory  \nresults_path: Path where search results will be stored\nparamspath: Output path for the parameter file. Can be a directory (creates searchparameters.json)  or full file path. Defaults to \"search_parameters.json\" in current directory.\nsimplified: If true (default), uses simplified template with essential parameters only.  If false, uses full template with all advanced options.\n\nReturns:\n\nString: Path to the newly created search parameters file\n\nTemplates used:\n\nSimplified: defaultSearchParamsSimplified.json (basic parameters)\nFull: defaultSearchParams.json (all advanced parameters)\n\nExample:\n\n# Create simplified parameter file\noutput_path = GetSearchParams(\n    \"/path/to/speclib.poin\",\n    \"/path/to/ms/data/dir\", \n    \"/path/to/results/dir\"\n)\n\n# Create full parameter file with custom output location\noutput_path = GetSearchParams(\n    \"/path/to/speclib.poin\",\n    \"/path/to/ms/data/dir\",\n    \"/path/to/results/dir\";\n    params_path = \"/custom/path/my_params.json\",\n    simplified = false\n)\n\n\n\n\n\n","category":"function"},{"location":"#Pioneer.BuildSpecLib","page":"Home","title":"Pioneer.BuildSpecLib","text":"BuildSpecLib(params_path::String)\n\nMain function to build a spectral library from parameters. Executes a series of steps:\n\nParameter validation and directory setup\nFragment bound detection\nRetention time prediction (optional)\nFragment prediction (optional)\nLibrary index building\n\nParameters:\n\nparams_path: Path to JSON configuration file containing library building parameters\n\nOutput:\n\nGenerates a spectral library in the specified output directory\nCreates a detailed log file with timing and performance metrics\nReturns nothing\n\n\n\n\n\n","category":"function"},{"location":"#Pioneer.GetBuildLibParams","page":"Home","title":"Pioneer.GetBuildLibParams","text":"GetBuildLibParams(out_dir::String, lib_name::String, fasta_inputs; \n                 params_path::Union{String, Missing} = missing,\n                 regex_codes::Union{Missing, Dict, Vector} = missing,\n                 simplified::Bool = true)\n\nCreates a library building parameter configuration file with user-specified paths and FASTA files.\n\nThe function loads default parameters from either the simplified or full JSON template  (from assets/example_config/) and creates a customized parameter file with the user's paths and automatically discovered FASTA files. All other parameters retain their default values and can be modified later.\n\nArguments:\n\nout_dir: Output directory path where the library will be built\nlib_name: Name for the spectral library (used for directory and file naming)\nfasta_inputs: FASTA file specification. Can be:\nA single directory path (String) - searches for .fasta/.fasta.gz files\nA single FASTA file path (String) \nAn array of directories and/or FASTA file paths\nparamspath: Output path for the parameter file. Can be a directory (creates buildspeclibparams.json) or full file path. Defaults to \"buildspeclib_params.json\" in current directory.\nregex_codes: Optional FASTA header regex patterns for protein annotation extraction. Can be:\nA single Dict with keys: \"accessions\", \"genes\", \"proteins\", \"organisms\" (applied to all FASTA files)\nA Vector of Dicts for positional mapping to fasta_inputs\nIf missing, uses default patterns from the template\nsimplified: If true (default), uses simplified template with essential parameters only. If false, uses full template with all advanced library building options.\n\nReturns:\n\nString: Path to the newly created library building parameters file\n\nTemplates used:\n\nSimplified: defaultBuildLibParamsSimplified.json (basic parameters)\nFull: defaultBuildLibParams.json (all advanced parameters)\n\nThe function automatically:\n\nDiscovers FASTA files in specified directories\nGenerates appropriate library names from FASTA filenames\nExpands regex patterns to match the number of FASTA files found\nValidates that all specified paths exist and are accessible\n\nExample:\n\n# Create simplified parameter file with directory of FASTA files\noutput_path = GetBuildLibParams(\n    \"/path/to/output\", \n    \"my_library\",\n    \"/path/to/fasta/directory\"\n)\n\n# Create full parameter file with specific FASTA files and custom regex\noutput_path = GetBuildLibParams(\n    \"/path/to/output\",\n    \"my_library\", \n    [\"/path/to/human.fasta\", \"/path/to/yeast.fasta\"];\n    params_path = \"/custom/path/build_params.json\",\n    regex_codes = Dict(\"accessions\" => \"^sp\\|(\\w+)\\|\", \"genes\" => \" GN=(\\S+)\"),\n    simplified = false\n)\n\n\n\n\n\n","category":"function"},{"location":"#Pioneer.convertMzML","page":"Home","title":"Pioneer.convertMzML","text":"convertMzML(mzml_dir::String; skip_scan_header::Bool=true)\n\nConvert mzML mass spectrometry data files to Arrow IPC format.\n\nTakes either a directory containing mzML files or a path to a single mzML file and converts them to  Arrow format, preserving scan data including m/z arrays, intensity arrays, and scan metadata.\n\nArguments\n\nmzml_dir::String: Path to either a directory containing mzML files or a path to a single mzML file\nskip_scan_header::Bool=true: When true, omits scan header information from the output to reduce file size\n\nReturns\n\nnothing\n\nOutput\n\nCreates Arrow (.arrow) files in the same directory as the input mzML files and with the same base filename.\n\nExamples\n\n# Convert all mzML files in a directory\nconvertMzML(\"path/to/mzml/files\")\n\n# Convert a single mzML file\nconvertMzML(\"path/to/single/file.mzML\")\n\n# Include scan headers in output\nconvertMzML(\"path/to/mzml/files\", skip_scan_header=false)\n\nNotes\n\nEach mzML file is converted to a corresponding Arrow IPC (.arrow) file in the same directory. This is particularly useful for Sciex data where direct .wiff/.wiff2 conversion is not supported\n\n\n\n\n\n","category":"function"},{"location":"development/global_prob_model_plan/#Global-Precursor-Probability-Model-—-Design-and-Implementation-Plan","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"","category":"section"},{"location":"development/global_prob_model_plan/#Goal","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Goal","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Replace the current heuristic :global_prob aggregation (log-odds of per-run :prec_prob) with a dedicated ML model trained at the precursor level. The model predicts a single experiment‑wide probability per precursor_idx using summary features aggregated across runs. The predicted probability becomes the new :global_prob and is assigned back to all PSM rows for that precursor_idx in merged_df and downstream artifacts.","category":"page"},{"location":"development/global_prob_model_plan/#Rationale","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Rationale","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"The present logodds(p, sqrt_n_runs) is simple and robust but cannot exploit richer cross‑run signals (e.g., score dispersion, coverage, consistency).\nA small tree‑based model (LightGBM, same backend used elsewhere) can learn non‑linearities and interactions from a compact, per‑precursor feature vector, improving separation of targets and decoys at the precursor level.","category":"page"},{"location":"development/global_prob_model_plan/#Scope","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Scope","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"In scope: New aggregation machinery, training pipeline, cross‑validated fit, prediction to all precursor_idx, and integration into ScoringSearch’s pipeline to populate :global_prob.\nOut of scope: Changes to per‑run :prec_prob estimation, downstream q‑value/PEP logic beyond switching :global_prob source, model persistence across projects.","category":"page"},{"location":"development/global_prob_model_plan/#Data-and-Features","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Data and Features","text":"","category":"section"},{"location":"development/global_prob_model_plan/#Source-tables","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Source tables","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"merged_df after Step 2/3 of ScoringSearch: has one row per (precursor_idx, ms_file_idx) with columns at minimum :precursor_idx, :prec_prob, :target, possibly :trace_prob, etc.","category":"page"},{"location":"development/global_prob_model_plan/#Aggregation-object-(custom-type)","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Aggregation object (custom type)","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Define a compact summary per precursor_idx. We will construct a Dictionary keyed by precursor_idx whose values are instances of a small immutable struct:","category":"page"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"struct GlobalPrecFeatures{N}\n    top_scores::NTuple{N, Float32}  # descending top-N :prec_prob across runs (pad with 0.0f0)\n    stats::NTuple{5, Float32}       # (mean, max, min, std, iqr)\n    counts::NTuple{3, Int32}        # (n_runs_with_score, n_runs_total, n_above_thresh)\n    deltas::NTuple{2, Float32}      # (top1 - top2, top2 - top3)\n    logodds_baseline::Float32       # current baseline (for ensembling/ablation)\nend","category":"page"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Notes:","category":"page"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"N (default 5) controls the number of highest scores retained; pad missing with zeros.\nn_runs_total equals the number of MS files considered for the experiment.\nn_above_thresh uses a configurable prec_prob threshold (e.g., 0.95) to capture run coverage quality.\niqr computed from observed per‑run scores for robustness; if <4 observations, fallback to 0.\nlogodds_baseline equals the existing logodds(p, sqrt_n_runs) result for a comparable reference feature.","category":"page"},{"location":"development/global_prob_model_plan/#Label-assignment","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Label assignment","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Precursor label target::Bool taken from library metadata (same across runs) via existing lookup on precursor_idx.","category":"page"},{"location":"development/global_prob_model_plan/#Feature-engineering-details","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Feature engineering details","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Sorting and padding for top_scores guarantee fixed‑length inputs for the model.\nStandardization is not required for tree‑based models, but we will clamp/clip probabilities into [eps, 1-eps] when derived metrics use logs (e.g., baseline log‑odds).","category":"page"},{"location":"development/global_prob_model_plan/#Training-Pipeline","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Training Pipeline","text":"","category":"section"},{"location":"development/global_prob_model_plan/#Dataset-construction","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Dataset construction","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Group merged_df by :precursor_idx and collect :prec_prob values (one per run where observed).\nBuild GlobalPrecFeatures{N} per group and record target.\nConvert the dictionary to a unique DataFrame with one row per precursor_idx and columns:\nprecursor_idx::UInt32, feature columns expanded from the struct, and target::Bool.","category":"page"},{"location":"development/global_prob_model_plan/#Cross-validation-(match-MBR/regular-scoring)","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Cross-validation (match MBR/regular scoring)","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Use Pioneer’s existing CV folds supplied by the spectral library: getCvFold(precursors, precursor_idx) (from StandardLibraryPrecursors.pid_to_cv_fold).\nThis mapping ensures all products of a given peptide (and all precursors belonging to the same protein group) end up in the same CV fold, consistent with MBR and regular scoring.\nTypical folds are two values (0 and 1). Perform CV by holding out one fold and training on the remaining fold(s), rotating across all unique fold values.","category":"page"},{"location":"development/global_prob_model_plan/#Model-choice","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Model choice","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"LightGBM only (binary logistic objective with probability output). No XGBoost.","category":"page"},{"location":"development/global_prob_model_plan/#Hyperparameters-(match-MBR-scoring;-not-user-configured)","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Hyperparameters (match MBR scoring; not user-configured)","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"num_leaves: 63\nmax_depth: 10\nlearning_rate: 0.15\nmindatain_leaf: 1\nfeature_fraction: 0.5\nbaggingfraction: 0.5 (with `baggingfreq = 1`)\nmingainto_split: 0.0\nnum_iterations: 200 (aligned with the final LightGBM round used in MBR)\nobjective: binary (logistic), outputs calibrated probabilities\nmetrics tracked: AUC, binary logloss (diagnostics only)","category":"page"},{"location":"development/global_prob_model_plan/#Calibration","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Calibration","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Tree outputs are calibrated probabilities under logistic loss; optionally add isotonic calibration on out‑of‑fold predictions if needed. Start without extra calibration for simplicity.","category":"page"},{"location":"development/global_prob_model_plan/#Evaluation","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Evaluation","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Report CV AUC and logloss.\nCompare ROC/AUC of model vs. baseline logodds_baseline to justify replacement.\nSanity check distributions on held‑out fold.","category":"page"},{"location":"development/global_prob_model_plan/#Integration-Points","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Integration Points","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Target file: src/Routines/SearchDIA/SearchMethods/ScoringSearch/ScoringSearch.jl","category":"page"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"After current computation of per‑run :prec_prob and before the current :global_prob calculation:\nBuild the per‑precursor dictionary of GlobalPrecFeatures{N}.\nTrain the model with CV and generate out‑of‑fold predictions for training diagnostics.\nFit final model on all data.\nGenerate per‑precursor predictions global_prob_pred[precursor_idx] using LightGBM trained with the library‑defined folds.\nAssign :global_prob = global_prob_pred[precursor_idx] to every row of merged_df by a single vectorized lookup or join.\nContinue existing pipeline steps that consume :global_prob (e.g., global q‑value spline and filtering).","category":"page"},{"location":"development/global_prob_model_plan/#Configuration","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Configuration","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Keep the feature behind a simple enable flag (e.g., optimization.machine_learning.global_prob_model.enabled; default false) and a small set of feature controls only (e.g., top_n, prec_prob_threshold).\nDo not expose model hyperparameters in JSON; they are fixed to the MBR settings listed above.","category":"page"},{"location":"development/global_prob_model_plan/#File/Type-Additions","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"File/Type Additions","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"New type: GlobalPrecFeatures{N} in an appropriate module (e.g., src/structs/GlobalProb.jl).\nNew utils: build_global_prec_features(merged_df; top_n, prob_thresh, n_runs) to compute the dictionary and DataFrame.\nNew ML helper: train_global_prob_model(df, folds, params) returning fitted model and OOF predictions.\nNew scorer: predict_global_prob(model, feature_df) -> Dict{UInt32, Float32} mapping precursor_idx to probability.","category":"page"},{"location":"development/global_prob_model_plan/#Pseudocode-Sketch","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Pseudocode Sketch","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"# After prec_prob computed per (precursor_idx, ms_file_idx)\nmerged_df = DataFrame(Arrow.Table(merged_scores_path))\nn_runs = length(getFilePaths(getMSData(ctx)))\n\ndict = Dict{UInt32, GlobalPrecFeatures{TOPN}}()\nlabels = Dict{UInt32, Bool}()\nfor g in groupby(merged_df, :precursor_idx)\n    pid = first(g.precursor_idx)\n    probs = sort!(Float32.(g.prec_prob); rev=true)\n    top_scores = ntuple(i -> i <= length(probs) ? probs[i] : 0.0f0, TOPN)\n    meanp = mean(probs); maxp = maximum(probs); minp = minimum(probs)\n    stdp = length(probs) > 1 ? std(probs) : 0f0\n    iqrp = length(probs) >= 4 ? (quantile(probs, 0.75f0) - quantile(probs, 0.25f0)) : 0f0\n    nabove = count(>=(prob_thresh), probs)\n    del12 = (top_scores[1] - top_scores[2])\n    del23 = (top_scores[2] - top_scores[3])\n    lodds = logodds(probs, floor(Int, sqrt(n_runs)))  # baseline\n    dict[pid] = GlobalPrecFeatures{TOPN}(\n        top_scores,\n        (meanp, maxp, minp, stdp, iqrp),\n        (length(probs), n_runs, nabove),\n        (del12, del23),\n        lodds,\n    )\n    labels[pid] = library_is_target(pid)\nend\n\nfeat_df = features_to_dataframe(dict, labels)  # 1 row per pid\nfolds = [getCvFold(getPrecursors(getSpecLib(ctx)), pid) for pid in feat_df.precursor_idx]\nmodel, oof_pred = train_global_prob_model(\n    feat_df,\n    folds;\n    num_iterations=200,\n    learning_rate=0.15,\n    num_leaves=63,\n    max_depth=10,\n    feature_fraction=0.5,\n    bagging_fraction=0.5,\n    min_data_in_leaf=1,\n    min_gain_to_split=0.0,\n)\nglobal_prob_map = predict_global_prob(model, feat_df)\n\n# Assign back to merged_df\nmerged_df.global_prob = [global_prob_map[pid] for pid in merged_df.precursor_idx]","category":"page"},{"location":"development/global_prob_model_plan/#Outputs-and-Artifacts","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Outputs and Artifacts","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Save training diagnostics to results folder: CV metrics, ROC curves, feature importances, and OOF prediction histogram.\nOptionally, persist the fitted model parameters (JSON) for reproducibility.","category":"page"},{"location":"development/global_prob_model_plan/#Testing-Plan","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Testing Plan","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Unit tests for feature builder: correct top‑N padding, stats, counts, baseline.\nSmall synthetic dataset: verify CV split stability and no leakage across groups.\nIntegration test: run ScoringSearch on test data with the feature flag enabled; check that :global_prob is replaced and that q‑value distributions remain sensible.\nRegression: compare target/decoy separation AUC vs. baseline log‑odds on the same data.","category":"page"},{"location":"development/global_prob_model_plan/#Risks-and-Mitigations","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Risks and Mitigations","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Leakage across folds via related precursors: Use protein or peptide grouping to partition folds.\nClass imbalance: Use class weights or scaleposweight; also track fdr_scale_factor for consistency with q‑value estimation.\nDependency sprawl: Use LightGBM only (already present); no new dependencies.\nOverfitting on small datasets: Use early stopping, CV monitoring, and keep feature set compact.","category":"page"},{"location":"development/global_prob_model_plan/#Rollout","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Rollout","text":"","category":"section"},{"location":"development/global_prob_model_plan/","page":"Global Precursor Probability Model — Design and Implementation Plan","title":"Global Precursor Probability Model — Design and Implementation Plan","text":"Implement behind optimization.machine_learning.global_prob_model.enabled (default false).\nLand feature with comprehensive tests and docs; keep old log‑odds as fallback.\nEvaluate on public and internal datasets; if improvements are consistent, flip default.","category":"page"}]
}
