name: Regression Tests

on:
  push:
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  regression:
    name: Julia ${{ matrix.version }} regression sweep
    runs-on: [self-hosted, Linux, pioneer-regression]
    timeout-minutes: 7200
    strategy:
      matrix:
        version: ['1.11']
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: Configure SSH for cluster access
        env:
          CLUSTER_SSH_KEY: ${{ secrets.CLUSTER_SSH_KEY }}
          CLUSTER_HOST: ${{ secrets.CLUSTER_HOST }}
        shell: bash
        run: |
          set -euo pipefail

          mkdir -p ~/.ssh
          install -m 600 /dev/null ~/.ssh/id_rsa
          # Normalize line endings and ensure a trailing newline
          printf '%s\n' "$CLUSTER_SSH_KEY" | tr -d '\r' > ~/.ssh/id_rsa

          # Preload host key to avoid interactive prompt
          ssh-keyscan -H "$CLUSTER_HOST" >> ~/.ssh/known_hosts

      - name: Prepare cluster workspace (configs, Pioneer, and depot)
        env:
          CLUSTER_HOST: ${{ secrets.CLUSTER_HOST }}
          CLUSTER_USERNAME: ${{ secrets.CLUSTER_USERNAME }}
          CLUSTER_RUN_ROOT: ${{ secrets.CLUSTER_RUN_ROOT }}
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          CLUSTER_ENTRAPMENT_REPO: ${{ secrets.CLUSTER_ENTRAPMENT_REPO }}
        shell: bash
        run: |
          set -euo pipefail

          remote_run_root_raw="${CLUSTER_RUN_ROOT:-/storage1/fs1/d.goldfarb/Active/Automation/Pioneer}"
          remote_run_root="${remote_run_root_raw%/}"
          run_suffix="run-${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}"
          remote_run_dir="${remote_run_root}/${run_suffix}"
          mount_run_root_raw="/mnt/ris/Active/Automation/Pioneer"
          mount_run_root="${mount_run_root_raw%/}"
          mount_run_dir="${mount_run_root}/${run_suffix}"
          pioneer_dir="${remote_run_dir}/pioneer"
          entrapment_dir="${remote_run_dir}/PioneerEntrapment.jl"
          depot_dir="${remote_run_dir}/julia-depot"
          pioneer_repo="https://github.com/${GITHUB_REPOSITORY}"
          entrapment_repo="${CLUSTER_ENTRAPMENT_REPO:-nwamsley1/PioneerEntrapment.jl}"
          entrapment_repo_url="$entrapment_repo"

          if [[ "$entrapment_repo" != http* ]]; then
            entrapment_repo_url="https://github.com/${entrapment_repo}"
          fi

          echo "CLUSTER_RUN_DIR=${remote_run_dir}" >> "$GITHUB_ENV"
          echo "CLUSTER_RUN_DIR_MOUNT=${mount_run_dir}" >> "$GITHUB_ENV"
          echo "CLUSTER_RUN_SUFFIX=${run_suffix}" >> "$GITHUB_ENV"
          echo "CLUSTER_PIONEER_DIR=${pioneer_dir}" >> "$GITHUB_ENV"
          echo "CLUSTER_ENTRAPMENT_DIR=${entrapment_dir}" >> "$GITHUB_ENV"
          echo "CLUSTER_DEPOT_DIR=${depot_dir}" >> "$GITHUB_ENV"

          ssh_options=(
            -o BatchMode=yes
            -o ServerAliveInterval=60
            -o ServerAliveCountMax=3
            -o ConnectTimeout=30
          )

          timeout 300 ssh "${ssh_options[@]}" "${CLUSTER_USERNAME}@${CLUSTER_HOST}" "RUN_ROOT='${remote_run_root}' RUN_DIR='${remote_run_dir}' PIONEER_DIR='${pioneer_dir}' DEPOT_DIR='${depot_dir}' PIONEER_REPO='${pioneer_repo}' TARGET_SHA='${GITHUB_SHA}' ENTRAPMENT_DIR='${entrapment_dir}' ENTRAPMENT_REPO_URL='${entrapment_repo_url}' bash -s" <<'EOF'
            set -eu
            mkdir -p "$RUN_DIR"
            cd "$RUN_DIR"

            if [ -d regression-configs/.git ]; then
              git -C regression-configs fetch --all
              git -C regression-configs reset --hard origin/main
            else
              git clone https://github.com/GoldfarbLab/pioneer-regression-configs regression-configs
            fi

            if [ -d "$PIONEER_DIR/.git" ]; then
              git -C "$PIONEER_DIR" fetch --all --tags
            else
              git clone "$PIONEER_REPO" "$PIONEER_DIR"
            fi

            git -C "$PIONEER_DIR" checkout --force "$TARGET_SHA"

            if [ -d "$ENTRAPMENT_DIR/.git" ]; then
              git -C "$ENTRAPMENT_DIR" fetch --all --tags
            else
              git clone "$ENTRAPMENT_REPO_URL" "$ENTRAPMENT_DIR"
            fi

            mkdir -p "$DEPOT_DIR"
          EOF

      - name: Submit regression search jobs on cluster
        env:
          CLUSTER_HOST: ${{ secrets.CLUSTER_HOST }}
          CLUSTER_USERNAME: ${{ secrets.CLUSTER_USERNAME }}
          CLUSTER_RUN_ROOT: ${{ secrets.CLUSTER_RUN_ROOT }}
          CLUSTER_RUN_DIR: ${{ env.CLUSTER_RUN_DIR }}
          CLUSTER_PIONEER_DIR: ${{ env.CLUSTER_PIONEER_DIR }}
          CLUSTER_ENTRAPMENT_DIR: ${{ env.CLUSTER_ENTRAPMENT_DIR }}
          REGRESSION_JOB_SCRIPT: ${{ vars.REGRESSION_JOB_SCRIPT }}
          SETUP_JOB_SCRIPT: ${{ vars.SETUP_JOB_SCRIPT }}
          METRICS_JOB_SCRIPT: ${{ vars.METRICS_JOB_SCRIPT }}
        shell: bash
        run: |
          set -euo pipefail

          remote_run_root="${CLUSTER_RUN_ROOT:-\$HOME/pioneer-regressions}"
          remote_run_dir="${CLUSTER_RUN_DIR:-${remote_run_root}/run-${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}}"
          mount_run_dir="${CLUSTER_RUN_DIR_MOUNT:-/mnt/ris/Active/Automation/Pioneer/run-${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}}"
          pioneer_dir="${CLUSTER_PIONEER_DIR:-${remote_run_dir}/pioneer}"
          entrapment_dir="${CLUSTER_ENTRAPMENT_DIR:-${remote_run_dir}/PioneerEntrapment.jl}"
          params_dir="${remote_run_dir}/regression-configs/params"
          regression_configs_dir="${remote_run_dir}/regression-configs"
          job_script_path="${remote_run_dir}/${REGRESSION_JOB_SCRIPT:-regression-configs/job_scripts/search_dia.bsub}"
          setup_script_path="${remote_run_dir}/${SETUP_JOB_SCRIPT:-regression-configs/job_scripts/setup.bsub}"
          metrics_script_path="${remote_run_dir}/${METRICS_JOB_SCRIPT:-regression-configs/job_scripts/metrics.bsub}"
          adjusted_params_dir="${remote_run_dir}/adjusted-params"
          run_suffix="run-${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}"

          ssh_options=(
            -o BatchMode=yes
            -o ServerAliveInterval=60
            -o ServerAliveCountMax=3
            -o ConnectTimeout=30
          )

          timeout 300 ssh "${ssh_options[@]}" "${CLUSTER_USERNAME}@${CLUSTER_HOST}" \
            "RUN_DIR='${remote_run_dir}' PIONEER_DIR='${pioneer_dir}' ENTRAPMENT_DIR='${entrapment_dir}' PARAMS_DIR='${params_dir}' REGRESSION_CONFIGS_DIR='${regression_configs_dir}' JOB_SCRIPT='${job_script_path}' SETUP_SCRIPT='${setup_script_path}' METRICS_SCRIPT='${metrics_script_path}' ADJUSTED_PARAMS_DIR='${adjusted_params_dir}' RUN_ID_SUFFIX='${run_suffix}' bash -s" <<'EOF'
            set -eu

            if [ ! -f "$SETUP_SCRIPT" ]; then
              echo "Missing setup job script: $SETUP_SCRIPT" >&2
              exit 1
            fi

            if [ ! -f "$JOB_SCRIPT" ]; then
              echo "Missing job script: $JOB_SCRIPT" >&2
              exit 1
            fi

            if [ ! -f "$METRICS_SCRIPT" ]; then
              echo "Missing metrics job script: $METRICS_SCRIPT" >&2
              exit 1
            fi

            setup_submit=$(PIONEER_DIR="$PIONEER_DIR" PIONEER_ENTRAPMENT_DIR="$ENTRAPMENT_DIR" PIONEER_REGRESSION_CONFIGS_DIR="$RUN_DIR/regression-configs" bsub < "$SETUP_SCRIPT")
            setup_job_id=$(printf '%s\n' "$setup_submit" | sed -n 's/Job <\([0-9]\+\)>.*$/\1/p' | head -n 1)

            if [ -z "$setup_job_id" ]; then
              echo "Failed to parse setup job ID from submission output:" >&2
              printf '%s\n' "$setup_submit" >&2
              exit 1
            fi

            param_list=$(find "$PARAMS_DIR" -type f -name 'search*.json' -print)

            if [ -z "$param_list" ]; then
              echo "No parameter JSON files found in $PARAMS_DIR" >&2
              exit 1
            fi

            mkdir -p "$ADJUSTED_PARAMS_DIR"
            job_id_dir="$RUN_DIR/job-ids"
            mkdir -p "$job_id_dir"
            manifest_file="$job_id_dir/expected_results.txt"
            : > "$manifest_file"

            printf '%s\n' "$param_list" | while IFS= read -r param_file; do
              rel_path="${param_file#${PARAMS_DIR}/}"
              dataset_dir_name="${rel_path%%/*}"
              search_name="$(basename "${rel_path%.*}")"
              adjusted_path="$ADJUSTED_PARAMS_DIR/$rel_path"
              mkdir -p "$(dirname "$adjusted_path")"

              results_path=$(sed -n 's/.*"results"[[:space:]]*:[[:space:]]*"\(.*\)".*/\1/p' "$param_file" | head -n 1)

              if [ -z "$results_path" ]; then
                echo "Failed to locate results path in $param_file" >&2
                exit 1
              fi

              base_results=${results_path%/}
              adjusted_results="${base_results}/${RUN_ID_SUFFIX}"
              dataset_name="$(basename "$(dirname "$adjusted_results")")"
              escaped_results=$(printf '%s\n' "$adjusted_results" | sed 's/[&/]/\\&/g')

              sed "s|\"results\"[[:space:]]*:[[:space:]]*\"[^\"]*\"|\"results\": \"${escaped_results}\"|" "$param_file" > "$adjusted_path"

              echo "${dataset_name}" >> "$manifest_file"

              search_submit=$(PIONEER_DIR="$PIONEER_DIR" PARAM_FILE="$adjusted_path" bsub -w "ended($setup_job_id)" < "$JOB_SCRIPT")
              search_job_id=$(printf '%s\n' "$search_submit" | sed -n 's/Job <\([0-9]\+\)>.*$/\1/p' | head -n 1)

              if [ -z "$search_job_id" ]; then
                echo "Failed to parse search job ID from submission output:" >&2
                printf '%s\n' "$search_submit" >&2
                exit 1
              fi

              echo "$search_job_id" >> "$job_id_dir/${dataset_dir_name}.search"
            done

            dataset_dirs=$(find "$PARAMS_DIR" -mindepth 1 -maxdepth 1 -type d -print)

            if [ -z "$dataset_dirs" ]; then
              echo "No dataset directories found under $PARAMS_DIR" >&2
              exit 1
            fi

            printf '%s\n' "$dataset_dirs" | while IFS= read -r dataset_dir; do
              dataset_name="$(basename "$dataset_dir")"
              job_list_file="$job_id_dir/${dataset_name}.search"
              metrics_file="$dataset_dir/metrics.json"
              # Experimental design files now live under params/<dataset>/experimental_design.json.
              exp_design_root="$PARAMS_DIR"

              if [ ! -f "$metrics_file" ]; then
                echo "Skipping dataset $dataset_name: missing metrics.json" >&2
                continue
              fi

              if [ ! -s "$job_list_file" ]; then
                echo "Skipping dataset $dataset_name: no search job IDs recorded" >&2
                continue
              fi

              dep_expr=""
              while IFS= read -r job_id; do
                [ -n "$job_id" ] || continue
                if [ -n "$dep_expr" ]; then
                  dep_expr="$dep_expr && "
                fi
                dep_expr="${dep_expr}ended(${job_id})"
              done < "$job_list_file"

              if [ -z "$dep_expr" ]; then
                echo "Skipping dataset $dataset_name: unable to build dependency expression" >&2
                continue
              fi

              param_dataset_dir="$ADJUSTED_PARAMS_DIR/$dataset_name"
              if [ ! -d "$param_dataset_dir" ]; then
                echo "Skipping dataset $dataset_name: adjusted params directory missing at $param_dataset_dir" >&2
                continue
              fi

              metrics_submit=$(\
                PIONEER_DIR="$PIONEER_DIR" \
                ENTRAPMENT_ANALYSES_PATH="$ENTRAPMENT_DIR" \
                PIONEER_PARAMS_DIR="$param_dataset_dir" \
                PIONEER_METRICS_FILE="$metrics_file" \
                PIONEER_EXPERIMENTAL_DESIGN="$exp_design_root" \
                PIONEER_THREE_PROTEOME_DESIGNS="$exp_design_root" \
                PIONEER_REGRESSION_CONFIGS_DIR="$REGRESSION_CONFIGS_DIR" \
                PIONEER_ARCHIVE_ROOT="$RUN_DIR" \
                bsub -w "$dep_expr" < "$METRICS_SCRIPT")

              metrics_job_id=$(printf '%s\n' "$metrics_submit" | sed -n 's/Job <\([0-9]\+\)>.*$/\1/p' | head -n 1)

              if [ -z "$metrics_job_id" ]; then
                echo "Failed to parse metrics job ID for dataset $dataset_name:" >&2
                printf '%s\n' "$metrics_submit" >&2
                exit 1
              fi
            done
          EOF

      - name: Wait for regression outputs on shared storage
        env:
          CLUSTER_RUN_DIR_MOUNT: ${{ env.CLUSTER_RUN_DIR_MOUNT }}
        shell: bash
        run: |
          set -euo pipefail

          run_dir="${CLUSTER_RUN_DIR_MOUNT:?CLUSTER_RUN_DIR_MOUNT not set}"
          manifest_path="$run_dir/job-ids/expected_results.txt"
          results_root="$run_dir/results"

          if [ ! -f "$manifest_path" ]; then
            echo "Expected manifest not found at $manifest_path" >&2
            exit 1
          fi

          mapfile -t manifest_entries < "$manifest_path"

          if [ "${#manifest_entries[@]}" -eq 0 ]; then
            echo "Manifest is empty; cannot determine expected results" >&2
            exit 1
          fi

          declare -A seen
          unique_entries=()
          for entry in "${manifest_entries[@]}"; do
            clean_entry="${entry%$'\r'}"
            [ -n "$clean_entry" ] || continue
            if [[ -z "${seen[$clean_entry]+x}" ]]; then
              seen[$clean_entry]=1
              unique_entries+=("$clean_entry")
            fi
          done

          total_expected=${#unique_entries[@]}
          poll_interval=300
          max_attempts=600
          attempt=1
          progress_log=()

          echo "Polling for ${total_expected} result paths:" | tee -a "$GITHUB_STEP_SUMMARY"
          printf '  %s\n' "${unique_entries[@]}" | tee -a "$GITHUB_STEP_SUMMARY"

          while :; do
            found=0
            missing=()

            for dataset in "${unique_entries[@]}"; do
              dataset_dir="$results_root/$dataset"

              metrics_glob="$dataset_dir/metrics_${dataset}_*.json"
              metrics_present=0
              if compgen -G "$metrics_glob" > /dev/null; then
                metrics_present=1
              fi

              log_present=""
              if compgen -G "$dataset_dir/*.log" > /dev/null; then
                log_present=1
              elif compgen -G "$dataset_dir/*.log.gz" > /dev/null; then
                log_present=1
              fi

              if [ "$metrics_present" -eq 1 ] || [ -n "$log_present" ]; then
                found=$((found + 1))
              else
                missing+=("$dataset")
              fi
            done

            timestamp=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
            progress_entry="[${timestamp}] Attempt ${attempt}: ${found}/${total_expected} ready"
            progress_log+=("$progress_entry")
            echo "$progress_entry" | tee -a "$GITHUB_STEP_SUMMARY"

            if [ "${#missing[@]}" -gt 0 ]; then
              echo "Missing entries:"
              printf '  %s\n' "${missing[@]}"
            fi

            if [ "${#missing[@]}" -eq 0 ]; then
              echo "All expected regression outputs detected."
              break
            fi

            if [ "$attempt" -ge "$max_attempts" ]; then
              echo "Timeout waiting for regression outputs after $((poll_interval * max_attempts / 60)) minutes" >&2
              printf 'Still missing:\n' >&2
              printf '  %s\n' "${missing[@]}" >&2
              {
                echo "## Cluster results polling"
                echo "- Run directory: $run_dir"
                echo "- Expected outputs: $total_expected"
                echo "- Status: timed out"
                echo "- Missing entries:"
                for entry in "${missing[@]}"; do
                  echo "  - $entry"
                done
                echo "- Progress log:"
                for line in "${progress_log[@]}"; do
                  echo "  - $line"
                done
              } >> "$GITHUB_STEP_SUMMARY"
              exit 1
            fi

            attempt=$((attempt + 1))
            sleep "$poll_interval"
          done

          {
            echo "## Cluster results polling"
            echo "- Run directory: $run_dir"
            echo "- Expected outputs: $total_expected"
            echo "- Status: completed"
            echo "- Progress log:"
            for line in "${progress_log[@]}"; do
              echo "  - $line"
            done
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Submit regression report job on cluster
        env:
          CLUSTER_HOST: ${{ secrets.CLUSTER_HOST }}
          CLUSTER_USERNAME: ${{ secrets.CLUSTER_USERNAME }}
          CLUSTER_RUN_DIR: ${{ env.CLUSTER_RUN_DIR }}
          CLUSTER_RUN_DIR_MOUNT: ${{ env.CLUSTER_RUN_DIR_MOUNT }}
          REPORT_JOB_SCRIPT: ${{ vars.REPORT_JOB_SCRIPT }}
        shell: bash
        run: |
          set -euo pipefail

          remote_run_dir="${CLUSTER_RUN_DIR:?CLUSTER_RUN_DIR not set}"
          report_script_path="${REPORT_JOB_SCRIPT:-regression-configs/job_scripts/metrics_report.bsub}"
          if [[ "$report_script_path" != /* ]]; then
            report_script_path="${remote_run_dir}/${report_script_path}"
          fi

          report_output_path="${remote_run_dir}/results/metrics_report.md"
          report_html_path="${remote_run_dir}/results/metrics_report.html"

          ssh_options=(
            -o BatchMode=yes
            -o ServerAliveInterval=60
            -o ServerAliveCountMax=3
            -o ConnectTimeout=30
          )

          report_submit="$(timeout 120 ssh "${ssh_options[@]}" "${CLUSTER_USERNAME}@${CLUSTER_HOST}" \
            "RUN_DIR='${remote_run_dir}' REPORT_SCRIPT='${report_script_path}' REPORT_OUTPUT='${report_output_path}' REPORT_HTML_OUTPUT='${report_html_path}' METRICS_RELEASE_ROOT='/storage1/fs1/d.goldfarb/Active/Automation/Pioneer/metrics/release' METRICS_DEVELOP_ROOT='/storage1/fs1/d.goldfarb/Active/Automation/Pioneer/metrics/develop' METRICS_CURRENT_ROOT='${remote_run_dir}/results' PIONEER_REGRESSION_CONFIGS_DIR='${remote_run_dir}/regression-configs' bash -s" <<'EOF'
              set -eu

              if [ ! -f "$REPORT_SCRIPT" ]; then
                echo "Missing report job script: $REPORT_SCRIPT" >&2
                exit 1
              fi

              submit_out=$(
                PIONEER_METRICS_RELEASE_ROOT="$METRICS_RELEASE_ROOT" \
                PIONEER_METRICS_DEVELOP_ROOT="$METRICS_DEVELOP_ROOT" \
                PIONEER_METRICS_CURRENT_ROOT="$METRICS_CURRENT_ROOT" \
                PIONEER_REPORT_OUTPUT="$REPORT_OUTPUT" \
                PIONEER_HTML_REPORT_OUTPUT="$REPORT_HTML_OUTPUT" \
                PIONEER_FDR_PLOTS_ROOT="$METRICS_CURRENT_ROOT" \
                PIONEER_REGRESSION_CONFIGS_DIR="$PIONEER_REGRESSION_CONFIGS_DIR" \
                bsub < "$REPORT_SCRIPT"
              )
              printf '%s' "$submit_out"
          EOF
          )"

          report_job_id=$(printf '%s\n' "$report_submit" | sed -n 's/Job <\([0-9]\+\)>.*$/\1/p' | head -n 1)

          if [ -z "$report_job_id" ]; then
            echo "Failed to parse report job ID from submission output:" >&2
            printf '%s\n' "$report_submit" >&2
            exit 1
          fi

          echo "REPORT_OUTPUT_PATH=${report_output_path}" >> "$GITHUB_ENV"
          echo "REPORT_HTML_PATH=${report_html_path}" >> "$GITHUB_ENV"
          echo "REPORT_JOB_ID=${report_job_id}" >> "$GITHUB_ENV"

          {
            echo "## Report job submission"
            echo "- Run directory: $remote_run_dir"
            echo "- Report job ID: $report_job_id"
            echo "- Report output: $report_output_path"
            echo "- HTML report output: $report_html_path"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Wait for regression report on shared storage
        env:
          CLUSTER_RUN_DIR: ${{ env.CLUSTER_RUN_DIR }}
          CLUSTER_RUN_DIR_MOUNT: ${{ env.CLUSTER_RUN_DIR_MOUNT }}
          REPORT_OUTPUT_PATH: ${{ env.REPORT_OUTPUT_PATH }}
          REPORT_HTML_PATH: ${{ env.REPORT_HTML_PATH }}
        shell: bash
        run: |
          set -euo pipefail

          run_dir="${CLUSTER_RUN_DIR_MOUNT:?CLUSTER_RUN_DIR_MOUNT not set}"
          report_path="${REPORT_OUTPUT_PATH:-${run_dir}/results/metrics_report.md}"
          report_path="${report_path/${CLUSTER_RUN_DIR:-}/${run_dir}}"
          html_report_path="${REPORT_HTML_PATH:-${run_dir}/results/metrics_report.html}"
          html_report_path="${html_report_path/${CLUSTER_RUN_DIR:-}/${run_dir}}"

          poll_interval=60
          max_attempts=180
          attempt=1

          while [ "$attempt" -le "$max_attempts" ]; do
            if [ -f "$report_path" ] && [ -f "$html_report_path" ]; then
              echo "Report found at $report_path"
              echo "REPORT_OUTPUT_PATH_MOUNT=${report_path}" >> "$GITHUB_ENV"
              echo "REPORT_HTML_PATH_MOUNT=${html_report_path}" >> "$GITHUB_ENV"
              {
                echo "## Regression report"
                echo "- Report path: $report_path"
                echo "- HTML report path: $html_report_path"
              } >> "$GITHUB_STEP_SUMMARY"
              exit 0
            fi

            echo "Waiting for report output (attempt ${attempt}/${max_attempts})..."
            attempt=$((attempt + 1))
            sleep "$poll_interval"
          done

          echo "Timeout waiting for regression report at $report_path" >&2
          exit 1

      - name: Upload regression HTML report artifact
        if: always()
        uses: actions/upload-artifact@v4
        env:
          REPORT_HTML_PATH_MOUNT: ${{ env.REPORT_HTML_PATH_MOUNT }}
        with:
          name: regression-efdr-html
          path: ${{ env.REPORT_HTML_PATH_MOUNT }}
          if-no-files-found: warn

      - name: Comment regression report on pull request
        if: always()
        uses: actions/github-script@v7
        env:
          CLUSTER_RUN_DIR: ${{ env.CLUSTER_RUN_DIR }}
          CLUSTER_RUN_DIR_MOUNT: ${{ env.CLUSTER_RUN_DIR_MOUNT }}
          REPORT_OUTPUT_PATH: ${{ env.REPORT_OUTPUT_PATH }}
          REPORT_HTML_PATH: ${{ env.REPORT_HTML_PATH }}
        with:
          script: |
            const fs = require('fs');
            const reportPathRaw = process.env.REPORT_OUTPUT_PATH;
            if (!reportPathRaw) {
              core.info('No report output path provided; skipping PR comment.');
              return;
            }
            const reportPath = reportPathRaw.replace(process.env.CLUSTER_RUN_DIR, process.env.CLUSTER_RUN_DIR_MOUNT);
            if (!fs.existsSync(reportPath)) {
              core.info(`Report not found at ${reportPath}; skipping PR comment.`);
              return;
            }
            const reportBody = fs.readFileSync(reportPath, 'utf8');
            const htmlPathRaw = process.env.REPORT_HTML_PATH || '';
            const htmlPath = htmlPathRaw.replace(process.env.CLUSTER_RUN_DIR || '', process.env.CLUSTER_RUN_DIR_MOUNT || '');
            const htmlAvailable = htmlPathRaw && fs.existsSync(htmlPath);
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const htmlNote = htmlAvailable
              ? `\n\n---\n\n**HTML eFDR report artifact**: \`regression-efdr-html\` (download from ${runUrl})`
              : `\n\n---\n\n**HTML eFDR report artifact**: not available (file not found)`;
            const body = `${reportBody}${htmlNote}`;
            const { owner, repo } = context.repo;
            const pulls = await github.rest.repos.listPullRequestsAssociatedWithCommit({
              owner,
              repo,
              commit_sha: context.sha,
            });
            if (!pulls.data.length) {
              core.info('No PR associated with this commit; skipping comment.');
              return;
            }
            await github.rest.issues.createComment({
              owner,
              repo,
              issue_number: pulls.data[0].number,
              body,
            });

      - name: Submit cluster cleanup job
        if: always()
        env:
          CLUSTER_HOST: ${{ secrets.CLUSTER_HOST }}
          CLUSTER_USERNAME: ${{ secrets.CLUSTER_USERNAME }}
          CLUSTER_RUN_DIR: ${{ env.CLUSTER_RUN_DIR }}
          CLUSTER_RUN_SUFFIX: ${{ env.CLUSTER_RUN_SUFFIX }}
          REPORT_JOB_ID: ${{ env.REPORT_JOB_ID }}
          CLEANUP_JOB_SCRIPT: ${{ vars.CLEANUP_JOB_SCRIPT }}
        shell: bash
        run: |
          set -euo pipefail

          remote_run_dir="${CLUSTER_RUN_DIR:?CLUSTER_RUN_DIR not set}"
          report_job_id="${REPORT_JOB_ID:-}"

          cleanup_script_path="${CLEANUP_JOB_SCRIPT:-regression-configs/job_scripts/cleanup.bsub}"
          if [[ "$cleanup_script_path" != /* ]]; then
            cleanup_script_path="${remote_run_dir}/${cleanup_script_path}"
          fi

          if [[ -n "${CLUSTER_RUN_SUFFIX:-}" ]] && [[ "${remote_run_dir##*/}" != "${CLUSTER_RUN_SUFFIX}" ]]; then
            echo "Run directory mismatch: expected suffix ${CLUSTER_RUN_SUFFIX}, got ${remote_run_dir##*/}" >&2
            exit 1
          fi

          ssh_options=(
            -o BatchMode=yes
            -o ServerAliveInterval=60
            -o ServerAliveCountMax=3
            -o ConnectTimeout=30
          )

          cleanup_submit="$(timeout 120 ssh "${ssh_options[@]}" "${CLUSTER_USERNAME}@${CLUSTER_HOST}" \
            "RUN_DIR='${remote_run_dir}' RUN_SUFFIX='${CLUSTER_RUN_SUFFIX:-}' CLEANUP_SCRIPT='${cleanup_script_path}' REPORT_JOB_ID='${report_job_id}' bash -s" <<'EOF'
              set -eu

              if [ ! -f "$CLEANUP_SCRIPT" ]; then
                echo "Missing cleanup job script: $CLEANUP_SCRIPT" >&2
                exit 1
              fi

              cleanup_wait=""
              if [ -n "$REPORT_JOB_ID" ]; then
                cleanup_wait="-w ended($REPORT_JOB_ID)"
              fi

              submit_out=$(RUN_DIR="$RUN_DIR" RUN_SUFFIX="$RUN_SUFFIX" bsub $cleanup_wait < "$CLEANUP_SCRIPT")
              printf '%s' "$submit_out"
          EOF
          )"

          cleanup_job_id=$(printf '%s\n' "$cleanup_submit" | sed -n 's/Job <\([0-9]\+\)>.*$/\1/p' | head -n 1)

          if [ -z "$cleanup_job_id" ]; then
            echo "Failed to parse cleanup job ID from submission output:" >&2
            printf '%s\n' "$cleanup_submit" >&2
            exit 1
          fi

          {
            echo "## Cleanup job submission"
            echo "- Run directory: $remote_run_dir"
            echo "- Cleanup job ID: $cleanup_job_id"
          } >> "$GITHUB_STEP_SUMMARY"
